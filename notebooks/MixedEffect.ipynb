{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The class is designed to be memory-efficient by using sparse matrices, which is particularly useful for datasets with many groups or sparse random effects design matrices.\n",
    "- The log-likelihood calculation is performed group by group, which allows for efficient computation even with large numbers of groups.\n",
    "- The use of L-BFGS-B optimization method allows for bounded optimization, which can help in ensuring that variance components remain positive.\n",
    "- The current implementation assumes that random effects are independent across groups. More complex covariance structures for random effects are not implemented in this version.\n",
    "- The prediction method currently only uses fixed effects. Incorporating random effects into predictions would require additional computation and would depend on the specific use case.\n",
    "- The class doesn't currently provide standard errors for the estimates. Implementing this would require computing the Hessian matrix or using a method like bootstrapping.\n",
    "- Diagnostic tools, such as residual plots or influence measures, are not included in this implementation but could be valuable additions for assessing model fit and assumptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.optimize import minimize\n",
    "import scipy.sparse.linalg as spla\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class MixedLM:\n",
    "    def __init__(self, endog, exog, groups, exog_re=None):\n",
    "        self.endog = np.asarray(endog).ravel()\n",
    "        self.exog = sparse.csr_matrix(exog)\n",
    "        self.groups = np.asarray(groups)\n",
    "        self.exog_re = sparse.csr_matrix(exog_re) if exog_re is not None else sparse.eye(self.exog.shape[0], format='csr')\n",
    "        \n",
    "        self.unique_groups = np.unique(groups)\n",
    "        self.n_groups = len(self.unique_groups)\n",
    "        self.n_re = self.exog_re.shape[1]\n",
    "        self.n_fe = self.exog.shape[1]\n",
    "        \n",
    "        # Standardize the fixed effects features\n",
    "        self.scaler = StandardScaler(with_mean=False)\n",
    "        self.exog = sparse.csr_matrix(self.scaler.fit_transform(self.exog.toarray()))\n",
    "        \n",
    "        # Pre-compute group-wise matrices\n",
    "        self.group_matrices = self._precompute_group_matrices()\n",
    "\n",
    "    def _precompute_group_matrices(self):\n",
    "        group_matrices = {}\n",
    "        for group in self.unique_groups:\n",
    "            mask = self.groups == group\n",
    "            group_matrices[group] = {\n",
    "                'X': self.exog[mask],\n",
    "                'Z': self.exog_re[mask],\n",
    "                'y': self.endog[mask]\n",
    "            }\n",
    "        return group_matrices\n",
    "\n",
    "    def log_likelihood_and_gradient(self, params):\n",
    "        beta = params[:self.n_fe]\n",
    "        sigma_re = np.exp(params[self.n_fe:self.n_fe + self.n_re])\n",
    "        sigma_e = np.exp(params[-1])\n",
    "\n",
    "        ll = 0\n",
    "        grad = np.zeros_like(params)\n",
    "\n",
    "        for group, matrices in self.group_matrices.items():\n",
    "            X, Z, y = matrices['X'], matrices['Z'], matrices['y']\n",
    "            \n",
    "            V = Z @ sparse.diags(sigma_re**2) @ Z.T + sigma_e**2 * sparse.eye(Z.shape[0])\n",
    "            r = y - X @ beta\n",
    "\n",
    "            # Use sparse Cholesky decomposition\n",
    "            try:\n",
    "                chol = spla.cholesky(V.tocsc())\n",
    "            except:\n",
    "                # If Cholesky fails, use LU decomposition as a fallback\n",
    "                lu = spla.splu(V.tocsc())\n",
    "                V_inv_r = lu.solve(r)\n",
    "                log_det = np.sum(np.log(np.abs(lu.U.diagonal())))\n",
    "            else:\n",
    "                V_inv_r = chol.solve_A(r)\n",
    "                log_det = 2 * np.sum(np.log(chol.L().diagonal()))\n",
    "            \n",
    "            # Log-likelihood\n",
    "            ll += -0.5 * (r.T @ V_inv_r + log_det)\n",
    "\n",
    "            # Gradient components\n",
    "            V_inv_X = chol.solve_A(X.toarray()) if 'chol' in locals() else lu.solve(X.toarray())\n",
    "            grad[:self.n_fe] += X.T @ V_inv_r\n",
    "            \n",
    "            V_inv = chol.solve_A(sparse.eye(V.shape[0]).toarray()) if 'chol' in locals() else lu.solve(sparse.eye(V.shape[0]).toarray())\n",
    "            P = V_inv - V_inv_r[:, np.newaxis] @ V_inv_r[np.newaxis, :]\n",
    "            \n",
    "            for j in range(self.n_re):\n",
    "                W_j = Z[:, j].toarray() @ Z[:, j].toarray().T\n",
    "                grad[self.n_fe + j] += 0.5 * np.trace(P @ W_j) * sigma_re[j]**2\n",
    "            \n",
    "            grad[-1] += 0.5 * np.trace(P) * sigma_e**2\n",
    "\n",
    "        # Add L2 regularization for random effects\n",
    "        lambda_reg = 0.1\n",
    "        ll -= 0.5 * lambda_reg * np.sum(sigma_re**2)\n",
    "        grad[self.n_fe:-1] -= lambda_reg * sigma_re**2\n",
    "\n",
    "        return -ll, -grad\n",
    "\n",
    "    def fit(self):\n",
    "        # Initialize fixed effects using ridge regression with sparse matrices\n",
    "        lambda_ridge = 0.1\n",
    "        A = self.exog.T @ self.exog + lambda_ridge * sparse.eye(self.n_fe)\n",
    "        b = self.exog.T @ self.endog\n",
    "        ridge_beta = spla.spsolve(A, b)\n",
    "        \n",
    "        # Initialize variance components\n",
    "        initial_re_var = np.var(self.endog) / (self.n_re + 1)\n",
    "        initial_params = np.concatenate([\n",
    "            ridge_beta,\n",
    "            np.log(np.random.uniform(0.5, 1.5, self.n_re) * initial_re_var),\n",
    "            [np.log(initial_re_var)]\n",
    "        ])\n",
    "\n",
    "        # Set bounds\n",
    "        bounds = [(None, None)] * self.n_fe + [(-10, 10)] * (self.n_re + 1)\n",
    "\n",
    "        result = minimize(\n",
    "            fun=lambda x: self.log_likelihood_and_gradient(x)[0],\n",
    "            x0=initial_params,\n",
    "            method='L-BFGS-B',\n",
    "            jac=lambda x: self.log_likelihood_and_gradient(x)[1],\n",
    "            bounds=bounds,\n",
    "            options={'maxiter': 100, 'ftol': 1e-6, 'gtol': 1e-6}\n",
    "        )\n",
    "        self.params = result.x\n",
    "        return result\n",
    "\n",
    "    def predict(self, exog_new):\n",
    "        beta = self.params[:self.n_fe]\n",
    "        exog_new_scaled = self.scaler.transform(exog_new)\n",
    "        return exog_new_scaled @ beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Dimensionality Reduction:\n",
    "# With one-hot encoding for all players at each position, you'd end up with a very high-dimensional dataset. Mixed-effects models can handle this more efficiently by treating players as random effects.\n",
    "# Partial Pooling:\n",
    "# Mixed-effects models perform partial pooling, which is particularly useful for players with limited data. It balances between individual estimates and overall position averages.\n",
    "# Hierarchical Structure:\n",
    "# You can model the nested structure of the data: plays within games, players within positions, etc.\n",
    "# Handling of Repeated Measures:\n",
    "# Since the same players appear in multiple plays, mixed-effects models can account for this non-independence in the data.\n",
    "# Separation of Fixed and Random Effects:\n",
    "# You can model position-specific effects as fixed, while individual player deviations are treated as random.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DATA_PATH = '../data/testing/ncaam_sample_data.csv'\n",
    "def load_data(data_path):\n",
    "    return pd.read_csv(data_path)\n",
    "\n",
    "m_data = load_data(DATA_PATH)\n",
    "scaler = MinMaxScaler()\n",
    "m_data['team_score']= m_data['team_score'].clip(36, 120)\n",
    "m_data['continuous_target'] = scaler.fit_transform(m_data['team_score'].values.reshape(-1, 1))\n",
    "m_data['continuous_target_2'] = m_data['team_fgm'].copy()/m_data['team_fga'].copy()\n",
    "m_data['date'] = pd.to_datetime(m_data['date'])\n",
    "m_data = m_data.sort_values(by=['date', 'team_name']).reset_index(drop=True)\n",
    "m_data = m_data.rename(columns={'team_name':'team','opp_name':'opponent'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>team_score</th>\n",
       "      <th>opp_score</th>\n",
       "      <th>is_home</th>\n",
       "      <th>numot</th>\n",
       "      <th>team_fgm</th>\n",
       "      <th>team_fga</th>\n",
       "      <th>team_fgm3</th>\n",
       "      <th>team_fga3</th>\n",
       "      <th>team_ftm</th>\n",
       "      <th>...</th>\n",
       "      <th>opp_ast</th>\n",
       "      <th>opp_to</th>\n",
       "      <th>opp_stl</th>\n",
       "      <th>opp_blk</th>\n",
       "      <th>opp_pf</th>\n",
       "      <th>team</th>\n",
       "      <th>opponent</th>\n",
       "      <th>date</th>\n",
       "      <th>continuous_target</th>\n",
       "      <th>continuous_target_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003</td>\n",
       "      <td>68</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>58</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>2002-11-14</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.465517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003</td>\n",
       "      <td>70</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>62</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>Memphis</td>\n",
       "      <td>Syracuse</td>\n",
       "      <td>2002-11-14</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.419355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003</td>\n",
       "      <td>62</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>2002-11-14</td>\n",
       "      <td>0.309524</td>\n",
       "      <td>0.415094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003</td>\n",
       "      <td>63</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>67</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>Syracuse</td>\n",
       "      <td>Memphis</td>\n",
       "      <td>2002-11-14</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.358209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003</td>\n",
       "      <td>55</td>\n",
       "      <td>81</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>E Washington</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2002-11-15</td>\n",
       "      <td>0.226190</td>\n",
       "      <td>0.434783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   season  team_score  opp_score  is_home  numot  team_fgm  team_fga  \\\n",
       "0    2003          68         62        0      0        27        58   \n",
       "1    2003          70         63        0      0        26        62   \n",
       "2    2003          62         68        0      0        22        53   \n",
       "3    2003          63         70        0      0        24        67   \n",
       "4    2003          55         81       -1      0        20        46   \n",
       "\n",
       "   team_fgm3  team_fga3  team_ftm  ...  opp_ast  opp_to  opp_stl  opp_blk  \\\n",
       "0          3         14        11  ...        8      18        9        2   \n",
       "1          8         20        10  ...        7      12        8        6   \n",
       "2          2         10        16  ...       13      23        7        1   \n",
       "3          6         24         9  ...       16      13        4        4   \n",
       "4          3         11        12  ...       12       9        9        3   \n",
       "\n",
       "   opp_pf          team   opponent       date  continuous_target  \\\n",
       "0      20       Alabama   Oklahoma 2002-11-14           0.380952   \n",
       "1      16       Memphis   Syracuse 2002-11-14           0.404762   \n",
       "2      22      Oklahoma    Alabama 2002-11-14           0.309524   \n",
       "3      18      Syracuse    Memphis 2002-11-14           0.321429   \n",
       "4      18  E Washington  Wisconsin 2002-11-15           0.226190   \n",
       "\n",
       "   continuous_target_2  \n",
       "0             0.465517  \n",
       "1             0.419355  \n",
       "2             0.415094  \n",
       "3             0.358209  \n",
       "4             0.434783  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_2022 = m_data[m_data['season'] == 2022].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  home_team away_team  home_score  away_score  days_rest_home  days_rest_away\n",
      "0   Team_19   Team_28          75          66               3               3\n",
      "1   Team_13   Team_17          60          92               2               2\n",
      "2    Team_9    Team_3          92          89               4               2\n",
      "3   Team_22   Team_28          72          61               2               3\n",
      "4   Team_20   Team_24          93          59               1               1\n",
      "        home_score   away_score  days_rest_home  days_rest_away\n",
      "count  1000.000000  1000.000000     1000.000000     1000.000000\n",
      "mean     81.572000    78.766000        2.498000        2.548000\n",
      "std      11.048932    11.054476        1.117696        1.120246\n",
      "min      51.000000    39.000000        1.000000        1.000000\n",
      "25%      74.000000    71.000000        1.750000        2.000000\n",
      "50%      81.000000    79.000000        2.000000        3.000000\n",
      "75%      89.000000    86.000000        3.000000        4.000000\n",
      "max     118.000000   115.000000        4.000000        4.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "def generate_basketball_data(n_games=1000, n_teams=30):\n",
    "    np.random.seed(42)  # for reproducibility\n",
    "    \n",
    "    # Generate team names\n",
    "    teams = [f\"Team_{i}\" for i in range(n_teams)]\n",
    "    \n",
    "    # Generate team strengths (will be used for both offense and defense)\n",
    "    team_strengths = stats.norm.rvs(loc=0, scale=5, size=n_teams)\n",
    "    \n",
    "    # Generate data\n",
    "    data = []\n",
    "    for _ in range(n_games):\n",
    "        home_team = np.random.choice(teams)\n",
    "        away_team = np.random.choice([t for t in teams if t != home_team])\n",
    "        \n",
    "        home_strength = team_strengths[teams.index(home_team)]\n",
    "        away_strength = team_strengths[teams.index(away_team)]\n",
    "        \n",
    "        # Home advantage (randomly between 2 to 4 points)\n",
    "        home_advantage = np.random.uniform(2, 4)\n",
    "        \n",
    "        # Generate scores (base score + team strength + random noise)\n",
    "        base_score = 80\n",
    "        home_score = int(base_score + home_strength + home_advantage + np.random.normal(0, 10))\n",
    "        away_score = int(base_score + away_strength + np.random.normal(0, 10))\n",
    "        \n",
    "        # Ensure non-negative scores\n",
    "        home_score = max(home_score, 0)\n",
    "        away_score = max(away_score, 0)\n",
    "        \n",
    "        # Additional features\n",
    "        days_rest_home = np.random.randint(1, 5)\n",
    "        days_rest_away = np.random.randint(1, 5)\n",
    "        \n",
    "        data.append({\n",
    "            'home_team': home_team,\n",
    "            'away_team': away_team,\n",
    "            'home_score': home_score,\n",
    "            'away_score': away_score,\n",
    "            'days_rest_home': days_rest_home,\n",
    "            'days_rest_away': days_rest_away\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate the data\n",
    "basketball_df = generate_basketball_data()\n",
    "\n",
    "# Display the first few rows\n",
    "print(basketball_df.head())\n",
    "\n",
    "# Basic statistics\n",
    "print(basketball_df.describe())\n",
    "\n",
    "# Save to CSV (optional)\n",
    "basketball_df.to_csv('basketball_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_mixedlm(df):\n",
    "    y = df['home_score'] - df['away_score']\n",
    "    X = np.column_stack([\n",
    "        df['days_rest_home'],\n",
    "        df['days_rest_away'],\n",
    "        np.ones(len(df))  # Intercept\n",
    "    ])\n",
    "    groups = df['home_team'].astype('category').cat.codes\n",
    "    \n",
    "    # We don't need to create Z here, as the class will handle it internally\n",
    "    return y, X, groups\n",
    "\n",
    "# Assuming basketball_df is already created\n",
    "y, X, groups = prepare_data_for_mixedlm(basketball_df)\n",
    "\n",
    "# Create and fit the model\n",
    "model = MixedLM(y, X, groups)\n",
    "\n",
    "start_time = time.time()\n",
    "result = model.fit()\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: ABNORMAL_TERMINATION_IN_LNSRCH\n",
       "  success: False\n",
       "   status: 2\n",
       "      fun: 1101759.854522504\n",
       "        x: [-2.562e-01  2.630e-01 ... -1.451e+00 -1.446e+00]\n",
       "      nit: 0\n",
       "      jac: [ 1.528e+03 -1.115e+03 ...  1.170e+02  6.150e+05]\n",
       "     nfev: 21\n",
       "     njev: 21\n",
       " hess_inv: <1004x1004 LbfgsInvHessProduct with dtype=float64>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
