{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy import sparse\n",
    "from datetime import datetime\n",
    "from scipy.linalg import solve\n",
    "from scipy.optimize import minimize\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_PATH = '../../data/testing/ncaam_sample_data.csv'\n",
    "def load_data(data_path):\n",
    "    return pd.read_csv(data_path)\n",
    "\n",
    "m_data = load_data(DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def optimize(self):\n",
    "        raise NotImplementedError(\"Subclasses must implement the optimize method.\")\n",
    "        \n",
    "class MasseyOptimizer:\n",
    "    def __init__(self, decay_type, protag_col='team', antag_col='opponent', stat_col='team_sq_score', meta_cols=['location'], min_protag_games=5):\n",
    "        self.decay_type = decay_type\n",
    "        self.protag_col = protag_col\n",
    "        self.antag_col = antag_col\n",
    "        self.stat_col = stat_col\n",
    "        self.meta_cols = meta_cols\n",
    "        self.min_protag_games = min_protag_games\n",
    "\n",
    "        if decay_type not in ['time', 'games', 'both']:\n",
    "            raise ValueError(\"decay_type must be 'time', 'games', or 'both'\")\n",
    "\n",
    "    def load_data(self, data=None, path=None):\n",
    "        if path:\n",
    "            self.data = pd.read_csv(path)\n",
    "        elif data is not None:\n",
    "            self.data = data.copy()\n",
    "        else:\n",
    "            raise ValueError(\"Either data or path must be provided\")\n",
    "        self._preprocess_data()\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        required_columns = [self.protag_col, self.antag_col, self.stat_col, 'date']\n",
    "        if not all(col in self.data.columns for col in required_columns):\n",
    "            raise ValueError(f\"Data must contain columns: {required_columns}\")\n",
    "\n",
    "        self.data['date'] = pd.to_datetime(self.data['date'])\n",
    "        self.data = self.data.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "        self.protags = sorted(self.data[self.protag_col].unique())\n",
    "        self.antags = sorted(self.data[self.antag_col].unique())\n",
    "\n",
    "        protag_map = {p: i for i, p in enumerate(self.protags)}\n",
    "        antag_map = {a: i for i, a in enumerate(self.antags)}\n",
    "\n",
    "        self.data['protag_idx'] = self.data[self.protag_col].map(protag_map)\n",
    "        self.data['antag_idx'] = self.data[self.antag_col].map(antag_map)\n",
    "\n",
    "        if len(self.data) <= 200:\n",
    "            raise ValueError(\"Not enough data to optimize (minimum 200 rows)\")\n",
    "\n",
    "    def _initialize_X(self, df, protags, antags):\n",
    "        df = df.copy()  # Create a copy to avoid SettingWithCopyWarning\n",
    "        num_protags = len(protags)\n",
    "        num_antags = len(antags)\n",
    "        \n",
    "        protag_map = {p: i for i, p in enumerate(protags)}\n",
    "        antag_map = {a: i for i, a in enumerate(antags)}\n",
    "        \n",
    "        df.loc[:, 'protag_idx'] = df[self.protag_col].map(protag_map).fillna(-1).astype(int)\n",
    "        df.loc[:, 'antag_idx'] = df[self.antag_col].map(antag_map).fillna(-1).astype(int)\n",
    "        \n",
    "        X = sparse.lil_matrix((len(df), num_protags + num_antags + len(self.meta_cols)))\n",
    "        valid_rows = (df['protag_idx'] != -1) & (df['antag_idx'] != -1)\n",
    "        X[valid_rows, df.loc[valid_rows, 'protag_idx']] = 1\n",
    "        X[valid_rows, df.loc[valid_rows, 'antag_idx'] + num_protags] = 1\n",
    "        \n",
    "        for i, col in enumerate(self.meta_cols):\n",
    "            X[:, -(i+1)] = df[col].values.reshape(-1, 1)\n",
    "        \n",
    "        return sparse.csr_matrix(X), df[valid_rows]\n",
    "\n",
    "    def _calculate_weights(self, train_data, test_date, halflife):\n",
    "        decay = np.exp(-np.log(2) / halflife)\n",
    "        time_diff = (test_date - train_data['date']).dt.total_seconds() / (24 * 3600)\n",
    "        weights = decay ** time_diff\n",
    "        return weights.values\n",
    "\n",
    "    def _fit_model(self, X_train, y_train, weights, l2):\n",
    "        W = sparse.diags(weights)\n",
    "        q = (X_train.T @ W @ X_train).toarray()\n",
    "        q += l2 * np.eye(q.shape[0]) * np.trace(q) / q.shape[0]\n",
    "        f = X_train.T @ W @ y_train\n",
    "        return solve(q, f, assume_a='pos')\n",
    "\n",
    "    def _predict_and_evaluate(self, X_test, y_test, coeffs, num_protags, num_antags):\n",
    "        offense_ratings = coeffs[:num_protags]\n",
    "        defense_ratings = coeffs[num_protags:num_protags+num_antags]\n",
    "        \n",
    "        X_test_ratings = np.column_stack([\n",
    "            offense_ratings[X_test[:, :num_protags].nonzero()[1]],\n",
    "            defense_ratings[X_test[:, num_protags:num_protags+num_antags].nonzero()[1] - num_protags]\n",
    "        ])\n",
    "        \n",
    "        linear_model = LinearRegression()\n",
    "        predictions = cross_val_predict(linear_model, X_test_ratings, y_test, cv=5)\n",
    "        mse = np.mean((y_test - predictions) ** 2)\n",
    "        return mse\n",
    "\n",
    "    def optimize(self, init_points=10, n_iter=30, num_test_dates=20, num_future_days=60, max_lookback=365*3, halflife_bounds=(10, 800), l2_bounds=(1e-9, 10)):\n",
    "        unique_dates = sorted(self.data['date'].unique())[10:]\n",
    "        test_dates = np.random.choice(unique_dates, size=num_test_dates, replace=False)\n",
    "\n",
    "        def objective(halflife, l2):\n",
    "            total_mse = 0\n",
    "            for test_date in test_dates:\n",
    "                train_data = self.data[(self.data['date'] >= test_date - pd.Timedelta(days=max_lookback)) & (self.data['date'] < test_date)].copy()\n",
    "                test_data = self.data[(self.data['date'] >= test_date) & (self.data['date'] <= test_date + pd.Timedelta(days=num_future_days))].copy()\n",
    "\n",
    "                if len(train_data) < 50 or len(test_data) < 50:\n",
    "                    continue\n",
    "\n",
    "                X_train, train_data = self._initialize_X(train_data, self.protags, self.antags)\n",
    "                X_test, test_data = self._initialize_X(test_data, self.protags, self.antags)\n",
    "\n",
    "                weights = self._calculate_weights(train_data, test_date, halflife)\n",
    "                coeffs = self._fit_model(X_train, train_data[self.stat_col].values, weights, l2)\n",
    "\n",
    "                mse = self._predict_and_evaluate(X_test, test_data[self.stat_col].values, coeffs, len(self.protags), len(self.antags))\n",
    "                total_mse += mse\n",
    "\n",
    "            return -total_mse / len(test_dates)\n",
    "\n",
    "        optimizer = BayesianOptimization(f=objective, pbounds={'halflife': halflife_bounds, 'l2': l2_bounds}, random_state=17)\n",
    "        optimizer.maximize(init_points=init_points, n_iter=n_iter)\n",
    "\n",
    "        best_params = optimizer.max['params']\n",
    "        best_mse = -optimizer.max['target']\n",
    "        return best_params['halflife'], best_params['l2'], best_mse\n",
    "\n",
    "    def get_ratings_for_dates(self, dates, halflife, l2, max_lookback=365*2.1):\n",
    "        offense_stats = []\n",
    "        defense_stats = []\n",
    "\n",
    "        for date in tqdm(dates):\n",
    "            date = pd.to_datetime(date)\n",
    "            train_data = self.data[(self.data['date'] >= date - pd.Timedelta(days=max_lookback)) & (self.data['date'] < date)].copy()\n",
    "            \n",
    "            if len(train_data) < 50:\n",
    "                print(f\"Minimum data threshold not met for date {date}\")\n",
    "                continue\n",
    "\n",
    "            X_train, train_data = self._initialize_X(train_data, self.protags, self.antags)\n",
    "            weights = self._calculate_weights(train_data, date, halflife)\n",
    "            coeffs = self._fit_model(X_train, train_data[self.stat_col].values, weights, l2)\n",
    "\n",
    "            num_protags = len(self.protags)\n",
    "            num_antags = len(self.antags)\n",
    "            \n",
    "            offense_ratings = coeffs[:num_protags]\n",
    "            defense_ratings = coeffs[num_protags:num_protags+num_antags]\n",
    "            meta_ratings = coeffs[num_protags+num_antags:]\n",
    "\n",
    "            offense_stats.append(pd.DataFrame({\n",
    "                'protag': self.protags,\n",
    "                self.stat_col: offense_ratings,\n",
    "                'date': date\n",
    "            }))\n",
    "\n",
    "            defense_stats.append(pd.DataFrame({\n",
    "                'antag': self.antags,\n",
    "                self.stat_col: defense_ratings,\n",
    "                'date': date\n",
    "            }))\n",
    "\n",
    "        return pd.concat(offense_stats), pd.concat(defense_stats)\n",
    "\n",
    "class MasseyMultiClassOptimizer(Optimizer):\n",
    "    def __init__(self, decay_type, protag_col='team', antag_col='opponent', stat_cols='team_sq_score', meta_cols=[]):\n",
    "        super().__init__()\n",
    "        assert(decay_type in ['time', 'games','both'])\n",
    "        self.decay_type = decay_type\n",
    "        self.protag_col = protag_col\n",
    "        self.antag_col = antag_col\n",
    "        self.stat_cols = stat_cols\n",
    "        self.meta_cols = meta_cols\n",
    "\n",
    "    def load_data(self, data, path=None):\n",
    "        if path is not None:\n",
    "            self.data = pd.read_csv(path)\n",
    "        else:\n",
    "            self.data = data\n",
    "        self.preprocess_data()\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        # Convert date column to datetime if needed\n",
    "        if isinstance(self.data['date'].iloc[0], str):\n",
    "            self.data['date'] = pd.to_datetime(self.data['date'])\n",
    "\n",
    "        # Sort data by date\n",
    "        self.data = self.data.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "        # Create a team/player list\n",
    "        self.protags = list(self.data[self.protag_col].unique())\n",
    "        self.antags = list(self.data[self.antag_col].unique())\n",
    "        self.protags = sorted(self.protags)\n",
    "        self.antags = sorted(self.antags)\n",
    "\n",
    "        self.num_protags = len(self.protags)\n",
    "        self.num_antags = len(self.antags)\n",
    "        self.num_stats = len(self.stat_cols)\n",
    "\n",
    "        self.data['protag_idx'] = self.data[self.protag_col].apply(lambda x: self.protags.index(x))\n",
    "        self.data['antag_idx'] = self.data[self.antag_col].apply(lambda x: self.antags.index(x))\n",
    "\n",
    "        if len(self.data[self.stat_cols[0]].unique()) > 10:\n",
    "            print('Warning: there are a lot of unique values in the stat col, can overload memory')\n",
    "\n",
    "        assert(len(self.data)>200), \"Not enough data to optimize\"\n",
    "\n",
    "    def initialize_X_train(self, train):\n",
    "\n",
    "        protags = sorted(train[self.protag_col].unique())\n",
    "        antags = sorted(train[self.antag_col].unique())\n",
    "        num_train_protags = len(protags)\n",
    "        num_train_antags = len(antags)\n",
    "\n",
    "        train['protag_idx'] = train[self.protag_col].apply(lambda x: protags.index(x))\n",
    "        train['antag_idx'] = train[self.antag_col].apply(lambda x: antags.index(x))\n",
    "\n",
    "        X_train = np.zeros((len(train), num_train_protags+num_train_antags+len(self.meta_cols)))\n",
    "        X_train[np.arange(len(train)), train['protag_idx']] = 1\n",
    "        X_train[np.arange(len(train)), train['antag_idx']+num_train_protags] = 1\n",
    "        for i, col in enumerate(self.meta_cols):\n",
    "            X_train[np.arange(len(train)), -1*i] = train[col]\n",
    "        \n",
    "        X_train = sparse.csr_matrix(X_train)\n",
    "        return X_train, protags, antags\n",
    "    \n",
    "    def initialize_X_test(self, test, protags, antags):\n",
    "\n",
    "        num_protags = len(protags)\n",
    "        num_antags = len(antags)\n",
    "        test['protag_idx'] = test[self.protag_col].apply(lambda x: protags.index(x) if x in protags else num_protags)\n",
    "        test['antag_idx'] = test[self.antag_col].apply(lambda x: antags.index(x) if x in antags else num_antags)\n",
    "        test = test.loc[test['protag_idx']<num_protags]\n",
    "        test = test.loc[test['antag_idx']<num_antags]\n",
    "        test_idx = test.index.values\n",
    "        test = test.reset_index(drop=True)\n",
    "        X_test = np.zeros((len(test), len(protags)+len(antags)+len(self.meta_cols)))\n",
    "        X_test[np.arange(len(test)), test['protag_idx']] = 1\n",
    "        X_test[np.arange(len(test)), test['antag_idx']+num_protags+1] = 1\n",
    "\n",
    "        for i, col in enumerate(self.meta_cols):\n",
    "            X_test[np.arange(len(test)), -1*i] = test[col]\n",
    "\n",
    "        X_test = sparse.csr_matrix(X_test)\n",
    "\n",
    "        return X_test, test_idx\n",
    "\n",
    "    def run_time_opt_scipy(self, init_points=10, n_iter=30, num_test_dates=20, num_future_days=60, max_lookback=365*2.5, halflife_bounds=(50, 800), l2_bounds=(1e-9, 1)):\n",
    "\n",
    "        # Select random test dates\n",
    "        unique_dates = self.data['date'].unique()\n",
    "        assert(len(unique_dates)>15), \"Not enough unique dates to test on\"\n",
    "        ## don't take from the first 10 or so dates\n",
    "        unique_dates = sorted(unique_dates)[10:]\n",
    "\n",
    "        test_dates = np.random.choice(unique_dates, size=num_test_dates, replace=False)\n",
    "        num_dates = len(unique_dates)\n",
    "\n",
    "        for test_date in test_dates:\n",
    "            train_data = self.data[(self.data['date'] >= test_date - pd.Timedelta(days=max_lookback)) & (self.data['date'] < test_date)].copy()\n",
    "            test_data = self.data[(self.data['date'] >= test_date) & (self.data['date'] <= test_date + pd.Timedelta(days=num_future_days))].copy()\n",
    "            if len(train_data) < 50 or len(test_data) < 50:\n",
    "                print(\"Not enough data for test date\", test_date)\n",
    "                test_dates = np.delete(test_dates, np.where(test_dates==test_date))\n",
    "\n",
    "        def time_bayes_objective(halflife, l2):\n",
    "            # offense_halflife, defense_halflife, meta_halflife_bounds, l2 = params\n",
    "            decay = np.exp(-np.log(2)/halflife)\n",
    "\n",
    "            for i, test_date in enumerate(test_dates):\n",
    "\n",
    "                # Filter data before the given date\n",
    "                X_train = self.data[(self.data['date'] >= test_date - pd.Timedelta(days=max_lookback)) & (self.data['date'] < test_date)].copy()\n",
    "                idx = X_train.index.values\n",
    "                if len(X_train) < 50:\n",
    "                    print(\"Minimum data threshold not met\")\n",
    "                    continue\n",
    "                X_train, protags, antags = self.initialize_X_train(X_train)\n",
    "                num_protags = len(protags)\n",
    "                num_antags = len(antags)\n",
    "                X_test = self.data[(self.data['date'] >= test_date) & (self.data['date'] <= test_date + pd.Timedelta(days=num_future_days))].copy()\n",
    "\n",
    "                if len(train_data) < 50 or len(test_data) < 50:\n",
    "                    continue\n",
    "\n",
    "                # Calculate the time differences in days between each game and the most recent game\n",
    "                dw = decay ** (np.datetime64(test_date)  - self.data.loc[idx, 'date'].values).reshape(-1).astype('timedelta64[D]').astype(int)\n",
    "                dw = dw/np.sum(dw)\n",
    "                Y_train = self.data.loc[idx, self.stat_cols].values\n",
    "\n",
    "                multilogit = LogisticRegression(C=1000, max_iter=5000,multi_class='ovr')\n",
    "                multilogit.fit(X_train, Y_train, sample_weight=dw)\n",
    "\n",
    "                # Split the ratings into intercept, offense, and defense ratings\n",
    "                offense_ratings = multilogit.coef_[:, :num_protags]\n",
    "                defense_ratings = multilogit.coef_[:, num_protags:num_protags+num_antags]\n",
    "            \n",
    "                # # Calculate ratings for the test date (without decay)\n",
    "                X_test, test_idx = self.initialize_X_test(test_data, protags, antags)\n",
    "                y_test = self.data.loc[test_idx, self.stat_cols].values\n",
    "                y_pred = multilogit.predict_proba(X_test)\n",
    "\n",
    "                # Evaluate the predictions using cross-entropy loss\n",
    "                cross_entropy = log_loss(y_test, y_pred)\n",
    "\n",
    "                # Print the evaluation result\n",
    "                print(f\"Test Cross-Entropy: {cross_entropy:.4f}\")\n",
    "\n",
    "\n",
    "            return -cross_entropy  \n",
    "        \n",
    "        best_decay_factor = None\n",
    "        best_l2 = None\n",
    "        best_mse = None\n",
    "\n",
    "        pbounds = {'halflife': halflife_bounds, 'l2': l2_bounds}\n",
    "        # pbounds = {'offense_halflife': offense_halflife_bounds, 'defense_halflife': defense_halflife_bounds, 'meta_halflife':meta_halflife_bounds, 'l2': l2_bounds}\n",
    "       \n",
    "        # Initialize the Bayesian Optimization object\n",
    "        optimizer = BayesianOptimization(f=time_bayes_objective, pbounds=pbounds, random_state=17)\n",
    "\n",
    "        # Perform the optimization\n",
    "        optimizer.maximize(init_points=init_points, n_iter=n_iter)\n",
    "\n",
    "        # Get the best parameters and correlation\n",
    "        best_params = optimizer.max['params']\n",
    "        best_halflife = best_params['offense_halflife']\n",
    "        best_l2 = best_params['l2']\n",
    "        best_correlation = -optimizer.max['target']\n",
    "\n",
    "        return best_halflife, best_l2, best_correlation\n",
    "\n",
    "    def run_full_time_opt(self, num_samples=25, num_test_dates=20, num_future_days=60, max_lookback=365*2, halflife_bounds=(50, 800), l2_bounds=(1e-8, 1)):\n",
    "\n",
    "        optimal_halflifes = []\n",
    "        optimal_l2s = []\n",
    "        best_errors = []\n",
    "        for i in tqdm(range(num_samples), total=num_samples):\n",
    "            best_halflife, best_l2, best_error = self.run_time_opt_scipy(13, 32, num_test_dates, num_future_days, max_lookback, halflife_bounds, l2_bounds)\n",
    "\n",
    "            optimal_halflifes.append(best_halflife)\n",
    "            optimal_l2s.append(best_l2)\n",
    "            best_errors.append(best_error)\n",
    "\n",
    "        return optimal_halflifes, optimal_l2s, best_errors\n",
    "\n",
    "    def get_ratings_for_dates(self, dates, halflife, l2, max_lookback=365*2.05):\n",
    "\n",
    "        num_dates = len(dates)\n",
    "        decay = np.exp(-np.log(2)/halflife)\n",
    "\n",
    "        offense_stats = []\n",
    "        defense_stats = []\n",
    "        for i, date in tqdm(enumerate(dates),total=num_dates):\n",
    "            # Filter data before the given date\n",
    "            X_train = self.data[(self.data['date'] >= date - pd.Timedelta(days=max_lookback)) & (self.data['date'] < date)].copy()\n",
    "            idx = X_train.index.values\n",
    "            if len(X_train) < 50:\n",
    "                print(\"Minimum data threshold not met\")\n",
    "                continue\n",
    "            X_train, protags, antags = self.initialize_X_train(X_train)\n",
    "            num_protags = len(protags)\n",
    "            num_antags = len(antags)\n",
    "\n",
    "            # Calculate the time differences in days between each game and the most recent game\n",
    "            dw = decay ** (np.datetime64(date)  - self.data.loc[idx, 'date'].values).reshape(-1).astype('timedelta64[D]').astype(int)\n",
    "            dw = dw/np.sum(dw)\n",
    "            Y_train = self.data.loc[idx, self.stat_cols].values\n",
    "\n",
    "            multilogit = LogisticRegression(C=1000, max_iter=5000,multi_class='ovr')\n",
    "            multilogit.fit(X_train, Y_train, sample_weight=dw)\n",
    "\n",
    "            # Split the ratings into intercept, offense, and defense ratings\n",
    "            offense_ratings = multilogit.coef_[:, :num_protags]\n",
    "            defense_ratings = multilogit.coef_[:, num_protags:num_protags+num_antags]\n",
    "\n",
    "            # Create DataFrames for offense ratings and defense ratings\n",
    "            offense_stat = pd.DataFrame(offense_ratings.T, columns=[f'stat_{i+1}' for i in range(offense_ratings.shape[0])])\n",
    "            offense_stat.insert(0, 'protag', protags)\n",
    "            offense_stat['date'] = date\n",
    "\n",
    "            defense_stat = pd.DataFrame(defense_ratings.T, columns=[f'defense_stat_{i+1}' for i in range(defense_ratings.shape[0])])\n",
    "            defense_stat.insert(0, 'antag', antags)\n",
    "            defense_stat['date'] = date\n",
    "\n",
    "            offense_stats.append(offense_stat)\n",
    "            defense_stats.append(defense_stat)\n",
    "\n",
    "        offense_stats = pd.concat(offense_stats).reset_index(drop=True)\n",
    "        defense_stats = pd.concat(defense_stats).reset_index(drop=True)\n",
    "        return offense_stats, defense_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>team_score</th>\n",
       "      <th>opp_score</th>\n",
       "      <th>is_home</th>\n",
       "      <th>numot</th>\n",
       "      <th>team_fgm</th>\n",
       "      <th>team_fga</th>\n",
       "      <th>team_fgm3</th>\n",
       "      <th>team_fga3</th>\n",
       "      <th>team_ftm</th>\n",
       "      <th>...</th>\n",
       "      <th>opp_or</th>\n",
       "      <th>opp_dr</th>\n",
       "      <th>opp_ast</th>\n",
       "      <th>opp_to</th>\n",
       "      <th>opp_stl</th>\n",
       "      <th>opp_blk</th>\n",
       "      <th>opp_pf</th>\n",
       "      <th>team_name</th>\n",
       "      <th>opp_name</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003</td>\n",
       "      <td>68</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>58</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>2002-11-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003</td>\n",
       "      <td>70</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>62</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>Memphis</td>\n",
       "      <td>Syracuse</td>\n",
       "      <td>2002-11-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003</td>\n",
       "      <td>62</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>2002-11-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003</td>\n",
       "      <td>63</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>67</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>Syracuse</td>\n",
       "      <td>Memphis</td>\n",
       "      <td>2002-11-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003</td>\n",
       "      <td>55</td>\n",
       "      <td>81</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>E Washington</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2002-11-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   season  team_score  opp_score  is_home  numot  team_fgm  team_fga  \\\n",
       "0    2003          68         62        0      0        27        58   \n",
       "1    2003          70         63        0      0        26        62   \n",
       "2    2003          62         68        0      0        22        53   \n",
       "3    2003          63         70        0      0        24        67   \n",
       "4    2003          55         81       -1      0        20        46   \n",
       "\n",
       "   team_fgm3  team_fga3  team_ftm  ...  opp_or  opp_dr  opp_ast  opp_to  \\\n",
       "0          3         14        11  ...      10      22        8      18   \n",
       "1          8         20        10  ...      20      25        7      12   \n",
       "2          2         10        16  ...      14      24       13      23   \n",
       "3          6         24         9  ...      15      28       16      13   \n",
       "4          3         11        12  ...      12      24       12       9   \n",
       "\n",
       "   opp_stl  opp_blk  opp_pf     team_name   opp_name        date  \n",
       "0        9        2      20       Alabama   Oklahoma  2002-11-14  \n",
       "1        8        6      16       Memphis   Syracuse  2002-11-14  \n",
       "2        7        1      22      Oklahoma    Alabama  2002-11-14  \n",
       "3        4        4      18      Syracuse    Memphis  2002-11-14  \n",
       "4        9        3      18  E Washington  Wisconsin  2002-11-15  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MO = MasseyOptimizer('time', protag_col='team_name', antag_col='opp_name', stat_col='team_score', meta_cols=['is_home'])\n",
    "MO.load_data(m_data.copy())\n",
    "# halflife, l2, mse = MO.run_time_opt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00, 10.37it/s]\n"
     ]
    }
   ],
   "source": [
    "rating_dates = sorted(m_data['date'].unique())[-15:]\n",
    "halflife = 150\n",
    "l2 = 1e-8\n",
    "offense_ratings, defense_ratings = MO.get_ratings_for_dates(rating_dates, halflife, l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protag</th>\n",
       "      <th>team_score</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Gonzaga</td>\n",
       "      <td>54.718075</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Iowa</td>\n",
       "      <td>51.461523</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>51.315833</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>48.939767</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>48.813532</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>St John's</td>\n",
       "      <td>47.974670</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Duke</td>\n",
       "      <td>47.787450</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Baylor</td>\n",
       "      <td>47.437041</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Kentucky</td>\n",
       "      <td>47.145549</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>North Carolina</td>\n",
       "      <td>47.125641</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             protag  team_score       date\n",
       "106         Gonzaga   54.718075 2022-04-04\n",
       "129            Iowa   51.461523 2022-04-04\n",
       "9           Arizona   51.315833 2022-04-04\n",
       "3           Alabama   48.939767 2022-04-04\n",
       "135          Kansas   48.813532 2022-04-04\n",
       "281       St John's   47.974670 2022-04-04\n",
       "77             Duke   47.787450 2022-04-04\n",
       "20           Baylor   47.437041 2022-04-04\n",
       "139        Kentucky   47.145549 2022-04-04\n",
       "209  North Carolina   47.125641 2022-04-04"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offense_ratings.drop_duplicates(subset=['protag'],keep='last').sort_values(by=['team_score'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MO.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.linalg import solve\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from bayes_opt import BayesianOptimization\n",
    "from tqdm import tqdm\n",
    "\n",
    "class MultiStatMasseyOptimizer:\n",
    "    def __init__(self, decay_type, protag_col='team', antag_col='opponent', stat_cols=['team_score', 'team_rebounds', 'team_assists'], meta_cols=['location'], min_protag_games=5):\n",
    "        self.decay_type = decay_type\n",
    "        self.protag_col = protag_col\n",
    "        self.antag_col = antag_col\n",
    "        self.stat_cols = stat_cols\n",
    "        self.meta_cols = meta_cols\n",
    "        self.min_protag_games = min_protag_games\n",
    "\n",
    "        if decay_type not in ['time', 'games', 'both']:\n",
    "            raise ValueError(\"decay_type must be 'time', 'games', or 'both'\")\n",
    "\n",
    "    def load_data(self, data=None, path=None):\n",
    "        if path:\n",
    "            self.data = pd.read_csv(path)\n",
    "        elif data is not None:\n",
    "            self.data = data.copy()\n",
    "        else:\n",
    "            raise ValueError(\"Either data or path must be provided\")\n",
    "        self._preprocess_data()\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        required_columns = [self.protag_col, self.antag_col] + self.stat_cols + ['date']\n",
    "        if not all(col in self.data.columns for col in required_columns):\n",
    "            raise ValueError(f\"Data must contain columns: {required_columns}\")\n",
    "\n",
    "        self.data['date'] = pd.to_datetime(self.data['date'])\n",
    "        self.data = self.data.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "        self.protags = sorted(self.data[self.protag_col].unique())\n",
    "        self.antags = sorted(self.data[self.antag_col].unique())\n",
    "\n",
    "        protag_map = {p: i for i, p in enumerate(self.protags)}\n",
    "        antag_map = {a: i for i, a in enumerate(self.antags)}\n",
    "\n",
    "        self.data['protag_idx'] = self.data[self.protag_col].map(protag_map)\n",
    "        self.data['antag_idx'] = self.data[self.antag_col].map(antag_map)\n",
    "\n",
    "        if len(self.data) <= 200:\n",
    "            raise ValueError(\"Not enough data to optimize (minimum 200 rows)\")\n",
    "\n",
    "    def _initialize_X(self, df, protags, antags):\n",
    "        df = df.copy()\n",
    "        num_protags = len(protags)\n",
    "        num_antags = len(antags)\n",
    "        \n",
    "        protag_map = {p: i for i, p in enumerate(protags)}\n",
    "        antag_map = {a: i for i, a in enumerate(antags)}\n",
    "        \n",
    "        df.loc[:, 'protag_idx'] = df[self.protag_col].map(protag_map).fillna(-1).astype(int)\n",
    "        df.loc[:, 'antag_idx'] = df[self.antag_col].map(antag_map).fillna(-1).astype(int)\n",
    "        \n",
    "        X = sparse.lil_matrix((len(df), num_protags + num_antags + len(self.meta_cols)))\n",
    "        valid_rows = (df['protag_idx'] != -1) & (df['antag_idx'] != -1)\n",
    "        X[valid_rows, df.loc[valid_rows, 'protag_idx']] = 1\n",
    "        X[valid_rows, df.loc[valid_rows, 'antag_idx'] + num_protags] = 1\n",
    "        \n",
    "        for i, col in enumerate(self.meta_cols):\n",
    "            X[:, -(i+1)] = df[col].values.reshape(-1, 1)\n",
    "        \n",
    "        return sparse.csr_matrix(X), df[valid_rows]\n",
    "\n",
    "    def _calculate_weights(self, train_data, test_date, halflife):\n",
    "        decay = np.exp(-np.log(2) / halflife)\n",
    "        time_diff = (test_date - train_data['date']).dt.total_seconds() / (24 * 3600)\n",
    "        weights = decay ** time_diff\n",
    "        return weights.values\n",
    "\n",
    "    def _fit_model(self, X_train, y_train, weights, l2):\n",
    "        W = sparse.diags(weights)\n",
    "        q = (X_train.T @ W @ X_train).toarray()\n",
    "        q += l2 * np.eye(q.shape[0]) * np.trace(q) / q.shape[0]\n",
    "        f = X_train.T @ W @ y_train\n",
    "        return solve(q, f, assume_a='pos')\n",
    "\n",
    "    def _predict_and_evaluate(self, X_test, y_test, coeffs, num_protags, num_antags):\n",
    "        offense_ratings = coeffs[:num_protags]\n",
    "        defense_ratings = coeffs[num_protags:num_protags+num_antags]\n",
    "        \n",
    "        X_test_ratings = np.column_stack([\n",
    "            offense_ratings[X_test[:, :num_protags].nonzero()[1]],\n",
    "            defense_ratings[X_test[:, num_protags:num_protags+num_antags].nonzero()[1] - num_protags]\n",
    "        ])\n",
    "        \n",
    "        linear_model = LinearRegression()\n",
    "        predictions = cross_val_predict(linear_model, X_test_ratings, y_test, cv=5)\n",
    "        mse = np.mean((y_test - predictions) ** 2)\n",
    "        return mse\n",
    "\n",
    "    def optimize(self, init_points=10, n_iter=30, num_test_dates=20, num_future_days=60, max_lookback=365*3, halflife_bounds=(10, 800), l2_bounds=(1e-9, 10)):\n",
    "        unique_dates = sorted(self.data['date'].unique())[10:]\n",
    "        test_dates = np.random.choice(unique_dates, size=num_test_dates, replace=False)\n",
    "\n",
    "        def objective(halflife, l2):\n",
    "            total_mse = 0\n",
    "            for test_date in test_dates:\n",
    "                train_data = self.data[(self.data['date'] >= test_date - pd.Timedelta(days=max_lookback)) & (self.data['date'] < test_date)].copy()\n",
    "                test_data = self.data[(self.data['date'] >= test_date) & (self.data['date'] <= test_date + pd.Timedelta(days=num_future_days))].copy()\n",
    "\n",
    "                if len(train_data) < 50 or len(test_data) < 50:\n",
    "                    continue\n",
    "\n",
    "                X_train, train_data = self._initialize_X(train_data, self.protags, self.antags)\n",
    "                X_test, test_data = self._initialize_X(test_data, self.protags, self.antags)\n",
    "\n",
    "                weights = self._calculate_weights(train_data, test_date, halflife)\n",
    "                \n",
    "                for stat_col in self.stat_cols:\n",
    "                    coeffs = self._fit_model(X_train, train_data[stat_col].values, weights, l2)\n",
    "                    mse = self._predict_and_evaluate(X_test, test_data[stat_col].values, coeffs, len(self.protags), len(self.antags))\n",
    "                    total_mse += mse\n",
    "\n",
    "            return -total_mse / (len(test_dates) * len(self.stat_cols))\n",
    "\n",
    "        optimizer = BayesianOptimization(f=objective, pbounds={'halflife': halflife_bounds, 'l2': l2_bounds}, random_state=17)\n",
    "        optimizer.maximize(init_points=init_points, n_iter=n_iter)\n",
    "\n",
    "        best_params = optimizer.max['params']\n",
    "        best_mse = -optimizer.max['target']\n",
    "        return best_params['halflife'], best_params['l2'], best_mse\n",
    "\n",
    "    def get_ratings_for_dates(self, dates, halflife, l2, max_lookback=365*2.1):\n",
    "        all_ratings = {stat: [] for stat in self.stat_cols}\n",
    "\n",
    "        for date in tqdm(dates):\n",
    "            date = pd.Timestamp(date)\n",
    "            train_data = self.data[(self.data['date'] >= date - pd.Timedelta(days=max_lookback)) & (self.data['date'] < date)].copy()\n",
    "            \n",
    "            if len(train_data) < 50:\n",
    "                print(f\"Minimum data threshold not met for date {date}\")\n",
    "                continue\n",
    "\n",
    "            X_train, train_data = self._initialize_X(train_data, self.protags, self.antags)\n",
    "            weights = self._calculate_weights(train_data, date, halflife)\n",
    "\n",
    "            num_protags = len(self.protags)\n",
    "            num_antags = len(self.antags)\n",
    "\n",
    "            for stat_col in self.stat_cols:\n",
    "                coeffs = self._fit_model(X_train, train_data[stat_col].values, weights, l2)\n",
    "                \n",
    "                offense_ratings = coeffs[:num_protags]\n",
    "                defense_ratings = coeffs[num_protags:num_protags+num_antags]\n",
    "                meta_ratings = coeffs[num_protags+num_antags:]\n",
    "\n",
    "                all_ratings[stat_col].append(pd.DataFrame({\n",
    "                    'team': self.protags,\n",
    "                    'offense_rating': offense_ratings,\n",
    "                    'defense_rating': defense_ratings,\n",
    "                    'date': date\n",
    "                }))\n",
    "\n",
    "        return {stat: pd.concat(ratings) for stat, ratings in all_ratings.items()}\n",
    "\n",
    "    def predict_game(self, protag, antag, date, meta_values=None):\n",
    "        if meta_values is None:\n",
    "            meta_values = [0] * len(self.meta_cols)\n",
    "        \n",
    "        train_data = self.data[self.data['date'] < date].copy()\n",
    "        X_train, train_data = self._initialize_X(train_data, self.protags, self.antags)\n",
    "        weights = self._calculate_weights(train_data, date, self.halflife)\n",
    "\n",
    "        predictions = {}\n",
    "        for stat_col in self.stat_cols:\n",
    "            coeffs = self._fit_model(X_train, train_data[stat_col].values, weights, self.l2)\n",
    "            \n",
    "            protag_idx = self.protags.index(protag)\n",
    "            antag_idx = self.antags.index(antag)\n",
    "            \n",
    "            offense_rating = coeffs[protag_idx]\n",
    "            defense_rating = coeffs[len(self.protags) + antag_idx]\n",
    "            meta_ratings = coeffs[len(self.protags) + len(self.antags):]\n",
    "            \n",
    "            prediction = offense_rating - defense_rating + np.dot(meta_ratings, meta_values)\n",
    "            predictions[stat_col] = prediction\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:02<00:00,  6.39it/s]\n"
     ]
    }
   ],
   "source": [
    "msmo = MultiStatMasseyOptimizer(\n",
    "    decay_type='time',\n",
    "    protag_col='team_name',\n",
    "    antag_col='opp_name',\n",
    "    stat_cols=['team_score', 'team_dr', 'team_ast'],\n",
    "    meta_cols=['is_home']\n",
    ")\n",
    "msmo.load_data(m_data)\n",
    "\n",
    "# Optimize the model\n",
    "# best_halflife, best_l2, best_mse = msmo.optimize()\n",
    "\n",
    "# Get ratings for specific dates\n",
    "dates = sorted(m_data['date'].unique())[-15:]\n",
    "best_halflife = 120\n",
    "best_l2 = 1e-8\n",
    "ratings = msmo.get_ratings_for_dates(dates, best_halflife, best_l2)\n",
    "\n",
    "# Access ratings for a specific stat\n",
    "score_ratings = ratings['team_score']\n",
    "rebound_ratings = ratings['team_dr']\n",
    "assist_ratings = ratings['team_ast']\n",
    "\n",
    "# Predict a specific game\n",
    "# prediction = msmo.predict_game('team_name', 'opp_name', date, meta_values=[1])\n",
    "# print(f\"Predicted score: {prediction['team_score']}\")\n",
    "# print(f\"Predicted rebounds: {prediction['team_dr']}\")\n",
    "# print(f\"Predicted assists: {prediction['team_ast']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team</th>\n",
       "      <th>offense_rating</th>\n",
       "      <th>defense_rating</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Gonzaga</td>\n",
       "      <td>54.461986</td>\n",
       "      <td>29.880230</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>51.622203</td>\n",
       "      <td>31.215235</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Iowa</td>\n",
       "      <td>51.344653</td>\n",
       "      <td>32.738985</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>49.109418</td>\n",
       "      <td>27.763761</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>48.898568</td>\n",
       "      <td>35.351730</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>St John's</td>\n",
       "      <td>48.040235</td>\n",
       "      <td>36.644418</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Duke</td>\n",
       "      <td>47.981441</td>\n",
       "      <td>29.429602</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Kentucky</td>\n",
       "      <td>47.464190</td>\n",
       "      <td>28.364729</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>North Carolina</td>\n",
       "      <td>47.326726</td>\n",
       "      <td>31.498682</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Baylor</td>\n",
       "      <td>47.286338</td>\n",
       "      <td>26.100549</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               team  offense_rating  defense_rating       date\n",
       "106         Gonzaga       54.461986       29.880230 2022-04-04\n",
       "9           Arizona       51.622203       31.215235 2022-04-04\n",
       "129            Iowa       51.344653       32.738985 2022-04-04\n",
       "135          Kansas       49.109418       27.763761 2022-04-04\n",
       "3           Alabama       48.898568       35.351730 2022-04-04\n",
       "281       St John's       48.040235       36.644418 2022-04-04\n",
       "77             Duke       47.981441       29.429602 2022-04-04\n",
       "139        Kentucky       47.464190       28.364729 2022-04-04\n",
       "209  North Carolina       47.326726       31.498682 2022-04-04\n",
       "20           Baylor       47.286338       26.100549 2022-04-04"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_ratings.drop_duplicates(subset=['team'],keep='last').sort_values(by=['offense_rating'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team</th>\n",
       "      <th>offense_rating</th>\n",
       "      <th>defense_rating</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abilene Chr</td>\n",
       "      <td>8.488406</td>\n",
       "      <td>13.495930</td>\n",
       "      <td>2022-03-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Air Force</td>\n",
       "      <td>7.502979</td>\n",
       "      <td>11.563780</td>\n",
       "      <td>2022-03-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akron</td>\n",
       "      <td>11.196717</td>\n",
       "      <td>9.447881</td>\n",
       "      <td>2022-03-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>14.763071</td>\n",
       "      <td>9.974514</td>\n",
       "      <td>2022-03-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama A&amp;M</td>\n",
       "      <td>11.663488</td>\n",
       "      <td>15.143592</td>\n",
       "      <td>2022-03-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>Wright St</td>\n",
       "      <td>11.632482</td>\n",
       "      <td>10.991622</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>14.754123</td>\n",
       "      <td>10.751527</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>Xavier</td>\n",
       "      <td>14.864636</td>\n",
       "      <td>10.995517</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>Yale</td>\n",
       "      <td>12.925246</td>\n",
       "      <td>12.342821</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>Youngstown St</td>\n",
       "      <td>10.212479</td>\n",
       "      <td>12.042673</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5445 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              team  offense_rating  defense_rating       date\n",
       "0      Abilene Chr        8.488406       13.495930 2022-03-11\n",
       "1        Air Force        7.502979       11.563780 2022-03-11\n",
       "2            Akron       11.196717        9.447881 2022-03-11\n",
       "3          Alabama       14.763071        9.974514 2022-03-11\n",
       "4      Alabama A&M       11.663488       15.143592 2022-03-11\n",
       "..             ...             ...             ...        ...\n",
       "358      Wright St       11.632482       10.991622 2022-04-04\n",
       "359        Wyoming       14.754123       10.751527 2022-04-04\n",
       "360         Xavier       14.864636       10.995517 2022-04-04\n",
       "361           Yale       12.925246       12.342821 2022-04-04\n",
       "362  Youngstown St       10.212479       12.042673 2022-04-04\n",
       "\n",
       "[5445 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rebound_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team</th>\n",
       "      <th>offense_rating</th>\n",
       "      <th>defense_rating</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abilene Chr</td>\n",
       "      <td>7.911145</td>\n",
       "      <td>5.084868</td>\n",
       "      <td>2022-03-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Air Force</td>\n",
       "      <td>6.826064</td>\n",
       "      <td>4.758409</td>\n",
       "      <td>2022-03-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akron</td>\n",
       "      <td>4.594847</td>\n",
       "      <td>4.313961</td>\n",
       "      <td>2022-03-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>8.669799</td>\n",
       "      <td>4.828603</td>\n",
       "      <td>2022-03-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama A&amp;M</td>\n",
       "      <td>2.031747</td>\n",
       "      <td>8.265675</td>\n",
       "      <td>2022-03-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>Wright St</td>\n",
       "      <td>6.449498</td>\n",
       "      <td>4.816999</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>5.337335</td>\n",
       "      <td>3.352986</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>Xavier</td>\n",
       "      <td>9.281279</td>\n",
       "      <td>5.520721</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>Yale</td>\n",
       "      <td>6.253933</td>\n",
       "      <td>5.539027</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>Youngstown St</td>\n",
       "      <td>4.439179</td>\n",
       "      <td>7.422963</td>\n",
       "      <td>2022-04-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5445 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              team  offense_rating  defense_rating       date\n",
       "0      Abilene Chr        7.911145        5.084868 2022-03-11\n",
       "1        Air Force        6.826064        4.758409 2022-03-11\n",
       "2            Akron        4.594847        4.313961 2022-03-11\n",
       "3          Alabama        8.669799        4.828603 2022-03-11\n",
       "4      Alabama A&M        2.031747        8.265675 2022-03-11\n",
       "..             ...             ...             ...        ...\n",
       "358      Wright St        6.449498        4.816999 2022-04-04\n",
       "359        Wyoming        5.337335        3.352986 2022-04-04\n",
       "360         Xavier        9.281279        5.520721 2022-04-04\n",
       "361           Yale        6.253933        5.539027 2022-04-04\n",
       "362  Youngstown St        4.439179        7.422963 2022-04-04\n",
       "\n",
       "[5445 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assist_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>team_score</th>\n",
       "      <th>opp_score</th>\n",
       "      <th>is_home</th>\n",
       "      <th>numot</th>\n",
       "      <th>team_fgm</th>\n",
       "      <th>team_fga</th>\n",
       "      <th>team_fgm3</th>\n",
       "      <th>team_fga3</th>\n",
       "      <th>team_ftm</th>\n",
       "      <th>...</th>\n",
       "      <th>opp_or</th>\n",
       "      <th>opp_dr</th>\n",
       "      <th>opp_ast</th>\n",
       "      <th>opp_to</th>\n",
       "      <th>opp_stl</th>\n",
       "      <th>opp_blk</th>\n",
       "      <th>opp_pf</th>\n",
       "      <th>team_name</th>\n",
       "      <th>opp_name</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003</td>\n",
       "      <td>68</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>58</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>2002-11-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003</td>\n",
       "      <td>70</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>62</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>Memphis</td>\n",
       "      <td>Syracuse</td>\n",
       "      <td>2002-11-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003</td>\n",
       "      <td>62</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>2002-11-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003</td>\n",
       "      <td>63</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>67</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>Syracuse</td>\n",
       "      <td>Memphis</td>\n",
       "      <td>2002-11-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003</td>\n",
       "      <td>55</td>\n",
       "      <td>81</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>E Washington</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2002-11-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   season  team_score  opp_score  is_home  numot  team_fgm  team_fga  \\\n",
       "0    2003          68         62        0      0        27        58   \n",
       "1    2003          70         63        0      0        26        62   \n",
       "2    2003          62         68        0      0        22        53   \n",
       "3    2003          63         70        0      0        24        67   \n",
       "4    2003          55         81       -1      0        20        46   \n",
       "\n",
       "   team_fgm3  team_fga3  team_ftm  ...  opp_or  opp_dr  opp_ast  opp_to  \\\n",
       "0          3         14        11  ...      10      22        8      18   \n",
       "1          8         20        10  ...      20      25        7      12   \n",
       "2          2         10        16  ...      14      24       13      23   \n",
       "3          6         24         9  ...      15      28       16      13   \n",
       "4          3         11        12  ...      12      24       12       9   \n",
       "\n",
       "   opp_stl  opp_blk  opp_pf     team_name   opp_name        date  \n",
       "0        9        2      20       Alabama   Oklahoma  2002-11-14  \n",
       "1        8        6      16       Memphis   Syracuse  2002-11-14  \n",
       "2        7        1      22      Oklahoma    Alabama  2002-11-14  \n",
       "3        4        4      18      Syracuse    Memphis  2002-11-14  \n",
       "4        9        3      18  E Washington  Wisconsin  2002-11-15  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "class MasseyMultiClassOptimizer:\n",
    "    def __init__(self, decay_type, protag_col='team', antag_col='opponent', stat_cols=['team_sq_score'], meta_cols=[], min_protag_games=5):\n",
    "        self.decay_type = decay_type\n",
    "        self.protag_col = protag_col\n",
    "        self.antag_col = antag_col\n",
    "        self.stat_cols = stat_cols if isinstance(stat_cols, list) else [stat_cols]\n",
    "        self.meta_cols = meta_cols\n",
    "        self.min_protag_games = min_protag_games\n",
    "\n",
    "        if decay_type not in ['time', 'games', 'both']:\n",
    "            raise ValueError(\"decay_type must be 'time', 'games', or 'both'\")\n",
    "\n",
    "        self._suppress_logistic_regression_warning()\n",
    "\n",
    "    @staticmethod\n",
    "    def _suppress_logistic_regression_warning():\n",
    "        warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.linear_model._logistic\")\n",
    "\n",
    "    def load_data(self, data=None, path=None):\n",
    "        if path:\n",
    "            self.data = pd.read_csv(path)\n",
    "        elif data is not None:\n",
    "            self.data = data.copy()\n",
    "        else:\n",
    "            raise ValueError(\"Either data or path must be provided\")\n",
    "        self._preprocess_data()\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        required_columns = [self.protag_col, self.antag_col] + self.stat_cols + ['date']\n",
    "        if not all(col in self.data.columns for col in required_columns):\n",
    "            raise ValueError(f\"Data must contain columns: {required_columns}\")\n",
    "\n",
    "        self.data['date'] = pd.to_datetime(self.data['date'])\n",
    "        self.data = self.data.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "        self.protags = sorted(self.data[self.protag_col].unique())\n",
    "        self.antags = sorted(self.data[self.antag_col].unique())\n",
    "\n",
    "        protag_map = {p: i for i, p in enumerate(self.protags)}\n",
    "        antag_map = {a: i for i, a in enumerate(self.antags)}\n",
    "\n",
    "        self.data['protag_idx'] = self.data[self.protag_col].map(protag_map)\n",
    "        self.data['antag_idx'] = self.data[self.antag_col].map(antag_map)\n",
    "\n",
    "        # Encode stat_cols if they're categorical\n",
    "        self.label_encoders = {}\n",
    "        for col in self.stat_cols:\n",
    "            if self.data[col].dtype == 'object':\n",
    "                le = LabelEncoder()\n",
    "                self.data[col] = le.fit_transform(self.data[col])\n",
    "                self.label_encoders[col] = le\n",
    "\n",
    "        if len(self.data) <= 200:\n",
    "            raise ValueError(\"Not enough data to optimize (minimum 200 rows)\")\n",
    "\n",
    "    def _initialize_X(self, df, protags, antags):\n",
    "        df = df.copy()\n",
    "        num_protags = len(protags)\n",
    "        num_antags = len(antags)\n",
    "        \n",
    "        protag_map = {p: i for i, p in enumerate(protags)}\n",
    "        antag_map = {a: i for i, a in enumerate(antags)}\n",
    "        \n",
    "        df['protag_idx'] = df[self.protag_col].map(protag_map).fillna(-1).astype(int)\n",
    "        df['antag_idx'] = df[self.antag_col].map(antag_map).fillna(-1).astype(int)\n",
    "        \n",
    "        X = np.zeros((len(df), num_protags + num_antags + len(self.meta_cols)))\n",
    "        valid_rows = (df['protag_idx'] != -1) & (df['antag_idx'] != -1)\n",
    "        X[valid_rows, df.loc[valid_rows, 'protag_idx']] = 1\n",
    "        X[valid_rows, df.loc[valid_rows, 'antag_idx'] + num_protags] = 1\n",
    "        \n",
    "        for i, col in enumerate(self.meta_cols):\n",
    "            X[:, -(i+1)] = df[col].values\n",
    "        \n",
    "        return csr_matrix(X), df[valid_rows]\n",
    "\n",
    "    def _calculate_weights(self, train_data, test_date, halflife):\n",
    "        decay = np.exp(-np.log(2) / halflife)\n",
    "        time_diff = (test_date - train_data['date']).dt.total_seconds() / (24 * 3600)\n",
    "        weights = decay ** time_diff\n",
    "        return weights.values / np.sum(weights.values)\n",
    "\n",
    "    def _fit_model(self, X_train, y_train, weights, l2):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "            base_model = LogisticRegression(C=1/l2, max_iter=5000, multi_class='ovr')\n",
    "            base_model.fit(X_train, y_train, sample_weight=weights)\n",
    "        return base_model\n",
    "\n",
    "    def _predict_and_evaluate(self, X_test, y_test, model):\n",
    "        y_pred = model.predict_proba(X_test)\n",
    "        return log_loss(y_test, y_pred)\n",
    "\n",
    "    def optimize(self, init_points=10, n_iter=30, num_test_dates=20, num_future_days=60, max_lookback=365*3, halflife_bounds=(10, 800), l2_bounds=(1e-9, 10)):\n",
    "        unique_dates = sorted(self.data['date'].unique())[10:]\n",
    "        test_dates = np.random.choice(unique_dates, size=num_test_dates, replace=False)\n",
    "\n",
    "        def objective(halflife, l2):\n",
    "            total_loss = 0\n",
    "            for test_date in test_dates:\n",
    "                train_data = self.data[(self.data['date'] >= test_date - pd.Timedelta(days=max_lookback)) & (self.data['date'] < test_date)].copy()\n",
    "                test_data = self.data[(self.data['date'] >= test_date) & (self.data['date'] <= test_date + pd.Timedelta(days=num_future_days))].copy()\n",
    "\n",
    "                if len(train_data) < 50 or len(test_data) < 50:\n",
    "                    continue\n",
    "\n",
    "                X_train, train_data = self._initialize_X(train_data, self.protags, self.antags)\n",
    "                X_test, test_data = self._initialize_X(test_data, self.protags, self.antags)\n",
    "\n",
    "                weights = self._calculate_weights(train_data, test_date, halflife)\n",
    "                model = self._fit_model(X_train, train_data[self.stat_cols].values.ravel(), weights, l2)\n",
    "\n",
    "                y_test = test_data[self.stat_cols].values.ravel()\n",
    "                y_pred = model.predict_proba(X_test)\n",
    "                \n",
    "                # Explicitly calculate log loss for multiclass\n",
    "                loss = log_loss(y_test, y_pred, labels=np.unique(y_test))\n",
    "                total_loss += loss\n",
    "\n",
    "            return -total_loss / len(test_dates)\n",
    "\n",
    "        optimizer = BayesianOptimization(f=objective, pbounds={'halflife': halflife_bounds, 'l2': l2_bounds}, random_state=17)\n",
    "        optimizer.maximize(init_points=init_points, n_iter=n_iter)\n",
    "\n",
    "        best_params = optimizer.max['params']\n",
    "        best_loss = -optimizer.max['target']\n",
    "        return best_params['halflife'], best_params['l2'], best_loss\n",
    "\n",
    "    def get_ratings_for_dates(self, dates, halflife, l2, max_lookback=365*2.1):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "        dates = pd.to_datetime(dates)\n",
    "        all_ratings = []\n",
    "\n",
    "        for date in tqdm(dates):\n",
    "            train_data = self.data[(self.data['date'] >= date - pd.Timedelta(days=max_lookback)) & (self.data['date'] < date)]\n",
    "            \n",
    "            if len(train_data) < 50:\n",
    "                print(f\"Minimum data threshold not met for date {date}\")\n",
    "                continue\n",
    "\n",
    "            X_train, _ = self._initialize_X(train_data, self.protags, self.antags)\n",
    "            weights = self._calculate_weights(train_data, date, halflife)\n",
    "            model = self._fit_model(X_train, train_data[self.stat_cols].values.ravel(), weights, l2)\n",
    "\n",
    "            num_protags = len(self.protags)\n",
    "            num_antags = len(self.antags)\n",
    "            num_classes = model.coef_.shape[0]\n",
    "\n",
    "            daily_games = self.data[self.data['date'] == date]\n",
    "            \n",
    "            # Prepare rating matrices\n",
    "            offense_ratings = model.coef_[:, :num_protags]\n",
    "            defense_ratings = model.coef_[:, num_protags:num_protags+num_antags]\n",
    "\n",
    "            # Create mapping arrays\n",
    "            protag_idx = daily_games['protag_idx'].values\n",
    "            antag_idx = daily_games['antag_idx'].values\n",
    "\n",
    "            # Use advanced indexing to get ratings for each game\n",
    "            offense_game_ratings = offense_ratings[:, protag_idx].T\n",
    "            defense_game_ratings = defense_ratings[:, antag_idx].T\n",
    "\n",
    "            # Create a DataFrame with all ratings\n",
    "            ratings_df = pd.DataFrame(\n",
    "                np.column_stack([offense_game_ratings, defense_game_ratings]),\n",
    "                columns=(\n",
    "                    [f'offense_rating_{i}' for i in range(num_classes)] +\n",
    "                    [f'defense_rating_{i}' for i in range(num_classes)]\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Add metadata columns\n",
    "            ratings_df['date'] = date\n",
    "            ratings_df[self.protag_col] = daily_games[self.protag_col].values\n",
    "            ratings_df[self.antag_col] = daily_games[self.antag_col].values\n",
    "\n",
    "            all_ratings.append(ratings_df)\n",
    "\n",
    "        # Combine all dates\n",
    "        all_ratings_df = pd.concat(all_ratings, ignore_index=True)\n",
    "\n",
    "        # Split into offense and defense DataFrames\n",
    "        offense_cols = [self.protag_col, 'date'] + [f'offense_rating_{i}' for i in range(num_classes)]\n",
    "        defense_cols = [self.antag_col, 'date'] + [f'defense_rating_{i}' for i in range(num_classes)]\n",
    "\n",
    "        offense_stats = all_ratings_df[offense_cols].rename(columns={self.protag_col: 'team'})\n",
    "        defense_stats = all_ratings_df[defense_cols].rename(columns={self.antag_col: 'team'})\n",
    "\n",
    "        return offense_stats, defense_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>team_score</th>\n",
       "      <th>opp_score</th>\n",
       "      <th>is_home</th>\n",
       "      <th>numot</th>\n",
       "      <th>team_fgm</th>\n",
       "      <th>team_fga</th>\n",
       "      <th>team_fgm3</th>\n",
       "      <th>team_fga3</th>\n",
       "      <th>team_ftm</th>\n",
       "      <th>...</th>\n",
       "      <th>opp_dr</th>\n",
       "      <th>opp_ast</th>\n",
       "      <th>opp_to</th>\n",
       "      <th>opp_stl</th>\n",
       "      <th>opp_blk</th>\n",
       "      <th>opp_pf</th>\n",
       "      <th>team_name</th>\n",
       "      <th>opp_name</th>\n",
       "      <th>date</th>\n",
       "      <th>score_binned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003</td>\n",
       "      <td>68</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>58</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>2002-11-14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003</td>\n",
       "      <td>70</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>62</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>Memphis</td>\n",
       "      <td>Syracuse</td>\n",
       "      <td>2002-11-14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003</td>\n",
       "      <td>62</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>2002-11-14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003</td>\n",
       "      <td>63</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>67</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>Syracuse</td>\n",
       "      <td>Memphis</td>\n",
       "      <td>2002-11-14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003</td>\n",
       "      <td>55</td>\n",
       "      <td>81</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>E Washington</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2002-11-15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206555</th>\n",
       "      <td>2022</td>\n",
       "      <td>81</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>54</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>Villanova</td>\n",
       "      <td>2022-04-02</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206556</th>\n",
       "      <td>2022</td>\n",
       "      <td>81</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>Duke</td>\n",
       "      <td>2022-04-02</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206557</th>\n",
       "      <td>2022</td>\n",
       "      <td>65</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>57</td>\n",
       "      <td>13</td>\n",
       "      <td>31</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>Villanova</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>2022-04-02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206558</th>\n",
       "      <td>2022</td>\n",
       "      <td>72</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>66</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>2022-04-04</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206559</th>\n",
       "      <td>2022</td>\n",
       "      <td>69</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>73</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>2022-04-04</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206560 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        season  team_score  opp_score  is_home  numot  team_fgm  team_fga  \\\n",
       "0         2003          68         62        0      0        27        58   \n",
       "1         2003          70         63        0      0        26        62   \n",
       "2         2003          62         68        0      0        22        53   \n",
       "3         2003          63         70        0      0        24        67   \n",
       "4         2003          55         81       -1      0        20        46   \n",
       "...        ...         ...        ...      ...    ...       ...       ...   \n",
       "206555    2022          81         65        0      0        29        54   \n",
       "206556    2022          81         77        0      0        27        64   \n",
       "206557    2022          65         81        0      0        22        57   \n",
       "206558    2022          72         69        0      0        29        66   \n",
       "206559    2022          69         72        0      0        23        73   \n",
       "\n",
       "        team_fgm3  team_fga3  team_ftm  ...  opp_dr  opp_ast  opp_to  opp_stl  \\\n",
       "0               3         14        11  ...      22        8      18        9   \n",
       "1               8         20        10  ...      25        7      12        8   \n",
       "2               2         10        16  ...      24       13      23        7   \n",
       "3               6         24         9  ...      28       16      13        4   \n",
       "4               3         11        12  ...      24       12       9        9   \n",
       "...           ...        ...       ...  ...     ...      ...     ...      ...   \n",
       "206555         13         24        10  ...      17       12       9        3   \n",
       "206556         10         26        17  ...      25       12       4        7   \n",
       "206557         13         31         8  ...      25       18       7        4   \n",
       "206558          6         17         8  ...      29        9      13        2   \n",
       "206559          5         23        18  ...      27       11       9        6   \n",
       "\n",
       "        opp_blk  opp_pf       team_name        opp_name        date  \\\n",
       "0             2      20         Alabama        Oklahoma  2002-11-14   \n",
       "1             6      16         Memphis        Syracuse  2002-11-14   \n",
       "2             1      22        Oklahoma         Alabama  2002-11-14   \n",
       "3             4      18        Syracuse         Memphis  2002-11-14   \n",
       "4             3      18    E Washington       Wisconsin  2002-11-15   \n",
       "...         ...     ...             ...             ...         ...   \n",
       "206555        0      11          Kansas       Villanova  2022-04-02   \n",
       "206556        4      18  North Carolina            Duke  2022-04-02   \n",
       "206557        4       8       Villanova          Kansas  2022-04-02   \n",
       "206558        6      13          Kansas  North Carolina  2022-04-04   \n",
       "206559        5      16  North Carolina          Kansas  2022-04-04   \n",
       "\n",
       "        score_binned  \n",
       "0                  2  \n",
       "1                  2  \n",
       "2                  1  \n",
       "3                  1  \n",
       "4                  0  \n",
       "...              ...  \n",
       "206555             4  \n",
       "206556             4  \n",
       "206557             1  \n",
       "206558             2  \n",
       "206559             2  \n",
       "\n",
       "[206560 rows x 35 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_data['score_binned'] = pd.qcut(m_data['team_score'], 5, labels=False)\n",
    "m_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:03<00:00,  4.37it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MMCO = MasseyMultiClassOptimizer('time', protag_col='team_name', antag_col='opp_name', stat_cols=['score_binned'], meta_cols=['is_home'])\n",
    "MMCO.load_data(m_data.copy())\n",
    "# halflife, l2, mse = MMCO.optimize()\n",
    "\n",
    "rating_dates = sorted(m_data['date'].unique())[-15:]\n",
    "halflife = 120\n",
    "l2 = 1e-8\n",
    "ratings = MMCO.get_ratings_for_dates(rating_dates, halflife, l2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                 team       date  offense_rating_0  offense_rating_1  \\\n",
       " 0                 SMU 2022-03-11         -1.463397         -0.616961   \n",
       " 1      St Bonaventure 2022-03-11         -0.381402         -0.514551   \n",
       " 2             Seattle 2022-03-11          0.067428         -0.445615   \n",
       " 3        SE Louisiana 2022-03-11         -0.951221          0.411065   \n",
       " 4    UC Santa Barbara 2022-03-11          0.467863         -0.754431   \n",
       " ..                ...        ...               ...               ...   \n",
       " 277            Kansas 2022-04-02         -4.119647         -0.652892   \n",
       " 278    North Carolina 2022-04-02         -1.562677         -1.384586   \n",
       " 279         Villanova 2022-04-02         -1.045134         -0.716690   \n",
       " 280            Kansas 2022-04-04         -3.921869         -0.740778   \n",
       " 281    North Carolina 2022-04-04         -1.431219         -1.398705   \n",
       " \n",
       "      offense_rating_2  offense_rating_3  offense_rating_4  \n",
       " 0           -0.170408          1.396364          0.605474  \n",
       " 1            0.234513          0.534648          0.683143  \n",
       " 2            0.168315          0.513217         -0.124914  \n",
       " 3            0.143662         -0.469839          0.635937  \n",
       " 4            0.278123         -0.171924          0.956538  \n",
       " ..                ...               ...               ...  \n",
       " 277          0.404264          0.991382          2.192332  \n",
       " 278          0.244673          0.394923          2.344242  \n",
       " 279          0.009910          1.054191          1.110815  \n",
       " 280          0.371342          0.940165          2.376644  \n",
       " 281          0.188638          0.337962          2.425132  \n",
       " \n",
       " [282 rows x 7 columns],\n",
       "                team       date  defense_rating_0  defense_rating_1  \\\n",
       " 0             Tulsa 2022-03-11          0.548741          0.135534   \n",
       " 1          St Louis 2022-03-11          0.991323          0.279386   \n",
       " 2       Abilene Chr 2022-03-11          0.345031          0.284200   \n",
       " 3       New Orleans 2022-03-11         -1.618361         -0.644782   \n",
       " 4     Long Beach St 2022-03-11         -0.943508          0.656560   \n",
       " ..              ...        ...               ...               ...   \n",
       " 277       Villanova 2022-04-02          2.666678          0.754217   \n",
       " 278            Duke 2022-04-02          1.147729          0.275677   \n",
       " 279          Kansas 2022-04-02          1.539816          1.007434   \n",
       " 280  North Carolina 2022-04-04          1.103917          0.344857   \n",
       " 281          Kansas 2022-04-04          1.436926          1.187889   \n",
       " \n",
       "      defense_rating_2  defense_rating_3  defense_rating_4  \n",
       " 0            0.498730         -0.133881         -0.432618  \n",
       " 1            0.255500         -0.455627         -0.262507  \n",
       " 2           -0.398016          0.165648          0.285055  \n",
       " 3           -0.314666          0.638810          1.517375  \n",
       " 4            0.014235         -0.084445          0.900362  \n",
       " ..                ...               ...               ...  \n",
       " 277         -0.459547         -0.836373         -1.720879  \n",
       " 278          0.315101         -0.153146         -1.126607  \n",
       " 279         -0.534428         -0.356807         -1.253198  \n",
       " 280         -0.866299         -0.175754          0.017235  \n",
       " 281         -0.568063         -0.411497         -1.304205  \n",
       " \n",
       " [282 rows x 7 columns])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team</th>\n",
       "      <th>date</th>\n",
       "      <th>offense_rating_0</th>\n",
       "      <th>offense_rating_1</th>\n",
       "      <th>offense_rating_2</th>\n",
       "      <th>offense_rating_3</th>\n",
       "      <th>offense_rating_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>2022-04-04</td>\n",
       "      <td>-3.921869</td>\n",
       "      <td>-0.740778</td>\n",
       "      <td>0.371342</td>\n",
       "      <td>0.940165</td>\n",
       "      <td>2.376644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Kentucky</td>\n",
       "      <td>2022-03-17</td>\n",
       "      <td>-3.567765</td>\n",
       "      <td>-0.178213</td>\n",
       "      <td>-0.543485</td>\n",
       "      <td>0.514193</td>\n",
       "      <td>2.482238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Princeton</td>\n",
       "      <td>2022-03-13</td>\n",
       "      <td>-3.138864</td>\n",
       "      <td>-1.178203</td>\n",
       "      <td>-0.900765</td>\n",
       "      <td>0.528821</td>\n",
       "      <td>1.416427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>Davidson</td>\n",
       "      <td>2022-03-18</td>\n",
       "      <td>-3.060735</td>\n",
       "      <td>-0.249868</td>\n",
       "      <td>-0.112205</td>\n",
       "      <td>1.317391</td>\n",
       "      <td>0.572686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>Duke</td>\n",
       "      <td>2022-04-02</td>\n",
       "      <td>-2.874736</td>\n",
       "      <td>-1.635619</td>\n",
       "      <td>-0.155918</td>\n",
       "      <td>0.855029</td>\n",
       "      <td>2.466332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>2022-03-17</td>\n",
       "      <td>-2.810754</td>\n",
       "      <td>-0.019907</td>\n",
       "      <td>0.376329</td>\n",
       "      <td>0.600163</td>\n",
       "      <td>1.123640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>2022-03-24</td>\n",
       "      <td>-2.620607</td>\n",
       "      <td>-2.167322</td>\n",
       "      <td>-0.655918</td>\n",
       "      <td>-0.617368</td>\n",
       "      <td>3.915034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>S Dakota St</td>\n",
       "      <td>2022-03-17</td>\n",
       "      <td>-2.614745</td>\n",
       "      <td>-2.081051</td>\n",
       "      <td>-1.538762</td>\n",
       "      <td>-0.254470</td>\n",
       "      <td>2.485481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>Gonzaga</td>\n",
       "      <td>2022-03-24</td>\n",
       "      <td>-2.534668</td>\n",
       "      <td>-2.507969</td>\n",
       "      <td>-2.136097</td>\n",
       "      <td>-1.014751</td>\n",
       "      <td>4.951599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2022-03-18</td>\n",
       "      <td>-2.479474</td>\n",
       "      <td>-2.118683</td>\n",
       "      <td>-0.579715</td>\n",
       "      <td>1.040591</td>\n",
       "      <td>2.842992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              team       date  offense_rating_0  offense_rating_1  \\\n",
       "280         Kansas 2022-04-04         -3.921869         -0.740778   \n",
       "185       Kentucky 2022-03-17         -3.567765         -0.178213   \n",
       "143      Princeton 2022-03-13         -3.138864         -1.178203   \n",
       "219       Davidson 2022-03-18         -3.060735         -0.249868   \n",
       "276           Duke 2022-04-02         -2.874736         -1.635619   \n",
       "170  San Francisco 2022-03-17         -2.810754         -0.019907   \n",
       "252        Arizona 2022-03-24         -2.620607         -2.167322   \n",
       "161    S Dakota St 2022-03-17         -2.614745         -2.081051   \n",
       "254        Gonzaga 2022-03-24         -2.534668         -2.507969   \n",
       "206        Alabama 2022-03-18         -2.479474         -2.118683   \n",
       "\n",
       "     offense_rating_2  offense_rating_3  offense_rating_4  \n",
       "280          0.371342          0.940165          2.376644  \n",
       "185         -0.543485          0.514193          2.482238  \n",
       "143         -0.900765          0.528821          1.416427  \n",
       "219         -0.112205          1.317391          0.572686  \n",
       "276         -0.155918          0.855029          2.466332  \n",
       "170          0.376329          0.600163          1.123640  \n",
       "252         -0.655918         -0.617368          3.915034  \n",
       "161         -1.538762         -0.254470          2.485481  \n",
       "254         -2.136097         -1.014751          4.951599  \n",
       "206         -0.579715          1.040591          2.842992  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings[0].drop_duplicates(subset=['team'],keep='last').sort_values(by=['offense_rating_0'], ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | halflife  |    l2     |\n",
      "-------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m-1.6     \u001b[39m | \u001b[39m242.8    \u001b[39m | \u001b[39m5.306    \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m-1.595   \u001b[39m | \u001b[35m161.3    \u001b[39m | \u001b[35m0.679    \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m-1.6     \u001b[39m | \u001b[39m631.7    \u001b[39m | \u001b[39m6.563    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m-1.6     \u001b[39m | \u001b[39m513.6    \u001b[39m | \u001b[39m5.756    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m-1.6     \u001b[39m | \u001b[39m40.86    \u001b[39m | \u001b[39m3.578    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m-1.596   \u001b[39m | \u001b[39m757.1    \u001b[39m | \u001b[39m0.6004   \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m-1.601   \u001b[39m | \u001b[39m692.6    \u001b[39m | \u001b[39m8.773    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m-1.6     \u001b[39m | \u001b[39m50.44    \u001b[39m | \u001b[39m6.524    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m-1.6     \u001b[39m | \u001b[39m445.9    \u001b[39m | \u001b[39m5.975    \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m-1.599   \u001b[39m | \u001b[39m392.0    \u001b[39m | \u001b[39m2.83     \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m-1.6     \u001b[39m | \u001b[39m140.3    \u001b[39m | \u001b[39m9.315    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m-1.597   \u001b[39m | \u001b[39m160.7    \u001b[39m | \u001b[39m1.175    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m-1.597   \u001b[39m | \u001b[39m162.1    \u001b[39m | \u001b[39m0.9878   \u001b[39m |\n",
      "| \u001b[35m14       \u001b[39m | \u001b[35m-1.595   \u001b[39m | \u001b[35m161.6    \u001b[39m | \u001b[35m1e-09    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m-1.597   \u001b[39m | \u001b[39m758.7    \u001b[39m | \u001b[39m0.9217   \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m-1.599   \u001b[39m | \u001b[39m756.1    \u001b[39m | \u001b[39m2.006    \u001b[39m |\n",
      "| \u001b[35m17       \u001b[39m | \u001b[35m-1.594   \u001b[39m | \u001b[35m160.6    \u001b[39m | \u001b[35m1e-09    \u001b[39m |\n",
      "| \u001b[35m18       \u001b[39m | \u001b[35m-1.593   \u001b[39m | \u001b[35m159.0    \u001b[39m | \u001b[35m1e-09    \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m-1.598   \u001b[39m | \u001b[39m157.4    \u001b[39m | \u001b[39m1e-09    \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m-1.596   \u001b[39m | \u001b[39m159.7    \u001b[39m | \u001b[39m1e-09    \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m-1.595   \u001b[39m | \u001b[39m158.7    \u001b[39m | \u001b[39m0.5891   \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m-1.596   \u001b[39m | \u001b[39m158.5    \u001b[39m | \u001b[39m1e-09    \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m-1.595   \u001b[39m | \u001b[39m159.3    \u001b[39m | \u001b[39m0.6418   \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m-1.598   \u001b[39m | \u001b[39m158.7    \u001b[39m | \u001b[39m1.602    \u001b[39m |\n",
      "| \u001b[35m25       \u001b[39m | \u001b[35m-1.593   \u001b[39m | \u001b[35m162.7    \u001b[39m | \u001b[35m1e-09    \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m-1.595   \u001b[39m | \u001b[39m163.3    \u001b[39m | \u001b[39m1e-09    \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m-1.594   \u001b[39m | \u001b[39m162.8    \u001b[39m | \u001b[39m0.4869   \u001b[39m |\n",
      "| \u001b[35m28       \u001b[39m | \u001b[35m-1.588   \u001b[39m | \u001b[35m162.3    \u001b[39m | \u001b[35m0.1029   \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m-1.588   \u001b[39m | \u001b[39m162.3    \u001b[39m | \u001b[39m0.1029   \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m-1.592   \u001b[39m | \u001b[39m162.2    \u001b[39m | \u001b[39m0.3321   \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m-1.594   \u001b[39m | \u001b[39m162.1    \u001b[39m | \u001b[39m1e-09    \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m-1.59    \u001b[39m | \u001b[39m162.4    \u001b[39m | \u001b[39m0.1846   \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m-1.594   \u001b[39m | \u001b[39m162.4    \u001b[39m | \u001b[39m1e-09    \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m-1.59    \u001b[39m | \u001b[39m162.3    \u001b[39m | \u001b[39m0.2063   \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m-1.591   \u001b[39m | \u001b[39m162.6    \u001b[39m | \u001b[39m0.2215   \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m-1.593   \u001b[39m | \u001b[39m162.5    \u001b[39m | \u001b[39m0.3714   \u001b[39m |\n",
      "| \u001b[39m37       \u001b[39m | \u001b[39m-1.589   \u001b[39m | \u001b[39m162.2    \u001b[39m | \u001b[39m0.145    \u001b[39m |\n",
      "| \u001b[39m38       \u001b[39m | \u001b[39m-1.596   \u001b[39m | \u001b[39m162.7    \u001b[39m | \u001b[39m0.849    \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m-1.592   \u001b[39m | \u001b[39m160.4    \u001b[39m | \u001b[39m0.2983   \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m-1.591   \u001b[39m | \u001b[39m164.0    \u001b[39m | \u001b[39m0.2323   \u001b[39m |\n",
      "=================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(162.30269172938603),\n",
       " np.float64(0.10289347090477433),\n",
       " np.float64(1.5878800657274277))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MMCO.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "\n",
    "class MasseyProbOptimizer:\n",
    "    def __init__(self, decay_type, protag_col='team', antag_col='opponent', stat_col='team_sq_score', meta_cols=None, min_protag_games=5):\n",
    "        self.decay_type = decay_type\n",
    "        self.protag_col = protag_col\n",
    "        self.antag_col = antag_col\n",
    "        self.stat_col = stat_col\n",
    "        self.meta_cols = meta_cols if meta_cols is not None else []\n",
    "        self.min_protag_games = min_protag_games\n",
    "\n",
    "        if decay_type not in ['time', 'games', 'both']:\n",
    "            raise ValueError(\"decay_type must be 'time', 'games', or 'both'\")\n",
    "\n",
    "    def load_data(self, data=None, path=None):\n",
    "        if path:\n",
    "            self.data = pd.read_csv(path)\n",
    "        elif data is not None:\n",
    "            self.data = data.copy()\n",
    "        else:\n",
    "            raise ValueError(\"Either data or path must be provided\")\n",
    "        self._preprocess_data()\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        self.data['date'] = pd.to_datetime(self.data['date'])\n",
    "        self.data = self.data.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "        self.protags = sorted(self.data[self.protag_col].unique())\n",
    "        self.antags = sorted(self.data[self.antag_col].unique())\n",
    "\n",
    "        protag_map = {p: i for i, p in enumerate(self.protags)}\n",
    "        antag_map = {a: i for i, a in enumerate(self.antags)}\n",
    "\n",
    "        self.data['protag_idx'] = self.data[self.protag_col].map(protag_map)\n",
    "        self.data['antag_idx'] = self.data[self.antag_col].map(antag_map)\n",
    "\n",
    "        if len(self.data) <= 200:\n",
    "            raise ValueError(\"Not enough data to optimize (minimum 200 rows)\")\n",
    "\n",
    "    def _initialize_X(self, df, protags, antags):\n",
    "        num_protags = len(protags)\n",
    "        num_antags = len(antags)\n",
    "        num_features = num_protags + num_antags + len(self.meta_cols)\n",
    "        \n",
    "        protag_map = {p: i for i, p in enumerate(protags)}\n",
    "        antag_map = {a: i for i, a in enumerate(antags)}\n",
    "        \n",
    "        rows = np.arange(len(df))\n",
    "        protag_cols = df[self.protag_col].map(protag_map).fillna(-1).astype(int)\n",
    "        antag_cols = df[self.antag_col].map(antag_map).fillna(-1).astype(int) + num_protags\n",
    "        \n",
    "        valid_mask = (protag_cols != -1) & (antag_cols != num_protags - 1)\n",
    "        rows = rows[valid_mask]\n",
    "        protag_cols = protag_cols[valid_mask]\n",
    "        antag_cols = antag_cols[valid_mask]\n",
    "        \n",
    "        data = np.ones(len(rows) * 2)\n",
    "        cols = np.concatenate([protag_cols, antag_cols])\n",
    "        rows = np.concatenate([rows, rows])\n",
    "        \n",
    "        X = sparse.csr_matrix((data, (rows, cols)), shape=(len(df), num_features))\n",
    "        \n",
    "        if self.meta_cols:\n",
    "            meta_data = df[self.meta_cols].values\n",
    "            meta_sparse = sparse.csr_matrix(meta_data)\n",
    "            X = sparse.hstack([X, meta_sparse], format='csr')\n",
    "        \n",
    "        return X, df[valid_mask]\n",
    "\n",
    "    def _calculate_weights(self, train_data, test_date, halflife):\n",
    "        if self.decay_type == 'time':\n",
    "            time_diff = (test_date - train_data['date']).dt.total_seconds() / (24 * 3600)\n",
    "        elif self.decay_type == 'games':\n",
    "            time_diff = np.arange(len(train_data))[::-1]\n",
    "        else:  # 'both'\n",
    "            time_diff = (test_date - train_data['date']).dt.total_seconds() / (24 * 3600)\n",
    "            game_diff = np.arange(len(train_data))[::-1]\n",
    "            time_diff = time_diff * game_diff\n",
    "\n",
    "        decay = np.exp(-np.log(2) / halflife)\n",
    "        weights = decay ** time_diff\n",
    "        return weights / np.sum(weights)\n",
    "\n",
    "    def _fit_model(self, X_train, y_train, weights, l2):\n",
    "        model = Ridge(alpha=l2)\n",
    "        logit = lambda x: np.log(x/(1-x))\n",
    "        inverse_logit = lambda x: 1/(1+np.exp(-x))\n",
    "        sigmoid_transformer = TransformedTargetRegressor(regressor=model, func=inverse_logit, inverse_func=logit)\n",
    "        sigmoid_transformer.fit(X_train, y_train, sample_weight=weights)\n",
    "        return sigmoid_transformer\n",
    "\n",
    "    def _predict_and_evaluate(self, X_test, y_test, model):\n",
    "        predictions = model.predict(X_test)\n",
    "        nll = -y_test * np.log(predictions) - (1 - y_test) * np.log(1 - predictions)\n",
    "        return nll.mean()\n",
    "\n",
    "    def optimize(self, init_points=10, n_iter=30, num_test_dates=20, num_future_days=60, max_lookback=365*3, halflife_bounds=(10, 800), l2_bounds=(1e-9, 10)):\n",
    "        unique_dates = sorted(self.data['date'].unique())[10:]\n",
    "        test_dates = np.random.choice(unique_dates, size=num_test_dates, replace=False)\n",
    "\n",
    "        def objective(halflife, l2):\n",
    "            total_nll = 0\n",
    "            for test_date in test_dates:\n",
    "                train_data = self.data[(self.data['date'] >= test_date - pd.Timedelta(days=max_lookback)) & (self.data['date'] < test_date)].copy()\n",
    "                test_data = self.data[(self.data['date'] >= test_date) & (self.data['date'] <= test_date + pd.Timedelta(days=num_future_days))].copy()\n",
    "\n",
    "                if len(train_data) < 50 or len(test_data) < 50:\n",
    "                    continue\n",
    "\n",
    "                X_train, train_data = self._initialize_X(train_data, self.protags, self.antags)\n",
    "                X_test, test_data = self._initialize_X(test_data, self.protags, self.antags)\n",
    "\n",
    "                weights = self._calculate_weights(train_data, test_date, halflife)\n",
    "                model = self._fit_model(X_train, train_data[self.stat_col].values, weights, l2)\n",
    "\n",
    "                nll = self._predict_and_evaluate(X_test, test_data[self.stat_col].values, model)\n",
    "                total_nll += nll\n",
    "\n",
    "            return -total_nll / len(test_dates)\n",
    "\n",
    "        optimizer = BayesianOptimization(f=objective, pbounds={'halflife': halflife_bounds, 'l2': l2_bounds}, random_state=17)\n",
    "        optimizer.maximize(init_points=init_points, n_iter=n_iter)\n",
    "\n",
    "        best_params = optimizer.max['params']\n",
    "        best_nll = -optimizer.max['target']\n",
    "        return best_params['halflife'], best_params['l2'], best_nll\n",
    "\n",
    "    def get_ratings_for_dates(self, dates, halflife, l2, max_lookback=365*2.1):\n",
    "        offense_stats = []\n",
    "        defense_stats = []\n",
    "        meta_stats = []\n",
    "\n",
    "        for date in tqdm(dates):\n",
    "            date = pd.to_datetime(date)\n",
    "            train_data = self.data[(self.data['date'] >= date - pd.Timedelta(days=max_lookback)) & (self.data['date'] < date)].copy()\n",
    "            \n",
    "            if len(train_data) < 50:\n",
    "                print(f\"Minimum data threshold not met for date {date}\")\n",
    "                continue\n",
    "\n",
    "            X_train, train_data = self._initialize_X(train_data, self.protags, self.antags)\n",
    "            weights = self._calculate_weights(train_data, date, halflife)\n",
    "            model = self._fit_model(X_train, train_data[self.stat_col].values, weights, l2)\n",
    "\n",
    "            num_protags = len(self.protags)\n",
    "            num_antags = len(self.antags)\n",
    "            \n",
    "            coefficients = model.regressor_.coef_\n",
    "            offense_ratings = coefficients[:num_protags]\n",
    "            defense_ratings = coefficients[num_protags:num_protags+num_antags]\n",
    "            meta_ratings = coefficients[num_protags+num_antags:]\n",
    "\n",
    "            offense_stats.append(pd.DataFrame({\n",
    "                'protag': self.protags,\n",
    "                self.stat_col: offense_ratings,\n",
    "                'date': date\n",
    "            }))\n",
    "\n",
    "            defense_stats.append(pd.DataFrame({\n",
    "                'antag': self.antags,\n",
    "                self.stat_col: defense_ratings,\n",
    "                'date': date\n",
    "            }))\n",
    "\n",
    "            if self.meta_cols:\n",
    "                meta_dict = {'date': date}\n",
    "                meta_dict.update({col: [rating] for col, rating in zip(self.meta_cols, meta_ratings)})\n",
    "                meta_stats.append(pd.DataFrame(meta_dict))\n",
    "\n",
    "        offense_df = pd.concat(offense_stats, ignore_index=True)\n",
    "        defense_df = pd.concat(defense_stats, ignore_index=True)\n",
    "        \n",
    "        if self.meta_cols:\n",
    "            meta_df = pd.concat(meta_stats, ignore_index=True)\n",
    "        else:\n",
    "            meta_df = pd.DataFrame()\n",
    "\n",
    "        return offense_df, defense_df, meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_data['won_game'] = (m_data['team_score'] > m_data['opp_score']).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m halflife \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m120\u001b[39m\n\u001b[0;32m      7\u001b[0m l2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-8\u001b[39m\n\u001b[1;32m----> 8\u001b[0m offense_ratings, defense_ratings, meta_ratings \u001b[38;5;241m=\u001b[39m MProbO\u001b[38;5;241m.\u001b[39mget_ratings_for_dates(rating_dates, halflife, l2)\n",
      "Cell \u001b[1;32mIn[94], line 169\u001b[0m, in \u001b[0;36mMasseyProbOptimizer.get_ratings_for_dates\u001b[1;34m(self, dates, halflife, l2, max_lookback)\u001b[0m\n\u001b[0;32m    162\u001b[0m     defense_stats\u001b[38;5;241m.\u001b[39mappend(pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mantag\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mantags,\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstat_col: defense_ratings,\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m: date\n\u001b[0;32m    166\u001b[0m     }))\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_cols:\n\u001b[1;32m--> 169\u001b[0m         meta_stats\u001b[38;5;241m.\u001b[39mappend(pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m    170\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_cols,\n\u001b[0;32m    171\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstat_col: meta_ratings,\n\u001b[0;32m    172\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m: date\n\u001b[0;32m    173\u001b[0m         }))\n\u001b[0;32m    175\u001b[0m offense_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(offense_stats, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    176\u001b[0m defense_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(defense_stats, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Blake\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mc:\\Users\\Blake\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[38;5;241m=\u001b[39mdtype, typ\u001b[38;5;241m=\u001b[39mtyp, consolidate\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32mc:\\Users\\Blake\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m _extract_index(arrays)\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\Blake\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "\n",
    "MProbO = MasseyProbOptimizer('time', protag_col='team_name', antag_col='opp_name', stat_col='won_game', meta_cols=['is_home'])\n",
    "MProbO.load_data(m_data.copy())\n",
    "# halflife, l2, mse = MProbO.optimize()\n",
    "\n",
    "rating_dates = sorted(m_data['date'].unique())[-15:]\n",
    "halflife = 120\n",
    "l2 = 1e-8\n",
    "offense_ratings, defense_ratings, meta_ratings = MProbO.get_ratings_for_dates(rating_dates, halflife, l2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
